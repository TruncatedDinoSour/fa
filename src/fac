#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Fa programming language"""

import os
import sys
from dataclasses import dataclass
from enum import Enum, auto
from typing import (Any, Dict, Generator, Iterable, List, Optional, Set, Sized,
                    Tuple)
from warnings import filterwarnings as filter_warnings


class TokenType(Enum):
    PUSH_INT = auto()
    PUSH_STR = auto()
    MUTABILITY = auto()
    SYS = auto()
    DROP = auto()
    SWAP = auto()
    MACRO = auto()
    END = auto()
    EXPAND_MACRO = auto()
    INCLUDE = auto()
    OPERATOR = auto()
    COPY = auto()
    UNDEFINE = auto()
    NOP = auto()
    AS = auto()
    PUSH_NAME = auto()
    DEREF = auto()
    POINT = auto()
    UNNAME = auto()


class Mutability(Enum):
    RW = auto()
    RO = auto()


class Operator(Enum):
    ADD = auto()
    SUB = auto()
    MUL = auto()
    BOR = auto()
    BAND = auto()
    XOR = auto()


@dataclass
class Token:
    ttype: Optional[TokenType]
    tvalue: Any
    tpos: Tuple[str, int, int]  # file, line, ip

    def __str__(self) -> str:
        if self.ttype is None:
            return self.__repr__()

        return f"Token {self.ttype.name!r} at position {':'.join(map(str, self.tpos))!r} \
with value {self.tvalue!r}"


EXIT_OK: int = 0
EXIT_ER: int = 1
NULL_TOKEN: Token = Token(None, None, ("", 0, 0))
ANOTATION_CONF: Dict[str, bool] = {"enabled": True}

ASM_SNIPPETS: Dict[str, Dict[str, Tuple[str, ...]]] = {
    "fasm-x86_64-linux": {
        "registers": (
            "rax",
            "rdi",
            "rsi",
            "rdx",
            "r8",
            "r9",
            "r10",
            "r11",
            "r12",
            "r13",
            "r14",
            "r15",
        )
    }
}

MACROS: Dict[str, List[Token]] = {}
CTX: Set[str] = set()
INCLUDE_RESOLVE_ORDER: Tuple[str, ...] = (
    ".",
    "/usr/include/fa",
    "/usr/lib/fa",
    "/usr/local/include/fa",
    "/usr/local/lib/fa",
    os.path.expanduser("~/.local/include/fa"),
    os.path.expanduser("~/.local/lib/fa"),
)
SCOPES: List[str] = []
NAMES: Set[str] = set()


def error(msg: str, token: Token = NULL_TOKEN) -> None:
    print(f"ERROR: {':'.join(map(str, token.tpos))}: {msg}", file=sys.stderr)
    sys.exit(EXIT_ER)


def ei(msg: str, token: Token, ast: Sized, ip: int) -> None:
    if ip >= len(ast):
        error(f"IP overflow: {msg}", token)


def make_usable_asm(asm: Iterable[str]) -> Generator[Tuple[str, Token], None, None]:
    yield from ((line, NULL_TOKEN) for line in asm)


def anotate_assembly(
    assembly: Generator[Tuple[str, Token], None, None], comment_char: str
) -> Generator[str, None, None]:
    log("Anotating assembly")

    for asm in assembly:
        if asm[1] == NULL_TOKEN or not ANOTATION_CONF["enabled"]:
            yield asm[0]
            continue

        yield f"    {asm[0]}  {comment_char} {asm[1]}"


def parse_string(string: str, token: Token) -> Generator[int, None, None]:
    ip: int = 0

    while ip < len(string):
        char: str = string[ip]

        if char == "\\":
            ip += 1
            ei("Invalid escape sequence", token, string, ip)

            char = string[ip]

            if char == "n":
                yield 10
            elif char == "t":
                yield 9
            elif char == "b":
                yield 8
            elif char == "f":
                yield 12
            elif char == "r":
                yield 13
            elif char == "{":
                _code: str = ""

                ip += 1
                ei("No closing } in \\{ escape", token, string, ip)

                char = string[ip]

                while char != "}":
                    _code += char
                    ip += 1
                    char = string[ip]

                if not _code:
                    error("Empty escape code", token)

                yield int(_code)
            else:
                yield ord(char)
        else:
            yield ord(char)

        ip += 1


assert len(TokenType) == 19, "Unhandled token types in AST generation"
assert len(Mutability) == 2, "Unhandled mutability contexts in AST generation"
assert len(Operator) == 6, "Unhandled operators in AST generation"


def fa_to_ast(fa: str, line: int, file: str = "") -> Generator[Token, None, None]:
    words: List[str] = fa.split(" ")
    ip: int = 0

    def make_token(ttype: TokenType, tvalue: Any) -> Token:
        return Token(ttype, tvalue, (file, line, ip))

    while ip < len(words):
        word: str = words[ip].strip()
        old_ip: int = ip
        _position: str = ":".join(map(str, (file, line, ip)))

        if not word:
            ip += 1
            continue

        if word.startswith("--<") and word.endswith(">--"):
            ip += 1
            continue

        if "comment" in CTX and word.endswith(">--"):
            CTX.remove("comment")
            ip += 1
            continue

        if "comment" in CTX:
            ip += 1
            continue

        if word.startswith("--<") and "comment" not in CTX:
            CTX.add("comment")
            ip += 1
            continue

        if word.startswith("--"):
            break

        if word.startswith('"'):
            if word.startswith('"') and word.endswith('"') and word != '"':
                yield make_token(
                    TokenType.PUSH_STR,
                    word[1:-1],
                )
                ip += 1
                continue

            old_word = word

            ip += 1
            word = words[ip]

            _tmp_str: List[str] = [old_word, word]

            while not word.endswith('"'):
                ip += 1

                try:
                    word = words[ip].strip()
                except IndexError:
                    if len(_tmp_str) >= 2 and word != '"':
                        error(
                            f'Unclosed string at position {_position} at file {file!r} \
(starting at IP {old_ip}), did you forget to add a " at the end?',
                        )

                _tmp_str.append(word)

            yield make_token(
                TokenType.PUSH_STR,
                " ".join(_tmp_str)[1:-1],
            )
        elif word.isnumeric():
            if word.startswith("0") and len(word) != 1:
                error(
                    f"Word at position {_position}: numbers cannot start with 0, did you mean {word.lstrip('0')}"
                )

            yield make_token(TokenType.PUSH_INT, int(word))
        elif word.startswith("%"):
            yield make_token(TokenType.EXPAND_MACRO, word[1:])
        elif word.startswith("@"):
            yield make_token(TokenType.PUSH_NAME, word[1:])
        elif word == "rw":
            yield make_token(TokenType.MUTABILITY, Mutability.RW)
        elif word == "ro":
            yield make_token(TokenType.MUTABILITY, Mutability.RO)
        elif word == "sys":
            ip += 1
            ei(
                f"No register ammount supplied at position {_position}, did you forget to supply a number that is at least a 1",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            yield make_token(TokenType.SYS, int(word))
        elif word == "drop":
            yield make_token(TokenType.DROP, None)
        elif word == "swap":
            yield make_token(TokenType.SWAP, None)
        elif word == "macro":
            ip += 1
            ei(
                f"No macro name for definition supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            SCOPES.append(f"macro.{word}")
            yield make_token(TokenType.MACRO, word)
        elif word == "end":
            if not SCOPES:
                error(
                    f"No scope(s) to end at position {_position}, did you mean for e.g. 'macro'?",
                    NULL_TOKEN,
                )

            yield make_token(TokenType.END, SCOPES.pop())
        elif word == "include":
            _include_file: List[str] = []

            ip += 1
            ei(
                f"No include filename supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            if not word.startswith("'"):
                error("Include must start with '")

            _include_file.append(word[1:])

            while not word.endswith("'"):
                ip += 1
                ei(
                    f"Unclosed include filename supplied at position {_position}, did you forget to add a '",
                    NULL_TOKEN,
                    words,
                    ip,
                )

                word = words[ip].strip()
                _include_file.append(word)

            yield make_token(TokenType.INCLUDE, " ".join(_include_file)[:-1])
        elif word == "add":
            yield make_token(TokenType.OPERATOR, Operator.ADD)
        elif word == "sub":
            yield make_token(TokenType.OPERATOR, Operator.SUB)
        elif word == "mul":
            yield make_token(TokenType.OPERATOR, Operator.MUL)
        elif word == "copy":
            yield make_token(TokenType.COPY, None)
        elif word == "bor":
            yield make_token(TokenType.OPERATOR, Operator.BOR)
        elif word == "band":
            yield make_token(TokenType.OPERATOR, Operator.BAND)
        elif word == "xor":
            yield make_token(TokenType.OPERATOR, Operator.XOR)
        elif word == "undefine":
            ip += 1
            msg = f"No macro name for undefinition supplied at position {_position}"

            if len(MACROS):
                msg += f", did you mean `undefine {tuple(MACROS.keys())[-1]}"

            ei(
                msg,
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            yield make_token(TokenType.UNDEFINE, word)
        elif word == "nop":
            yield make_token(TokenType.NOP, None)
        elif word == "as":
            ip += 1
            ei(
                f"No name for defining supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            yield make_token(TokenType.AS, word)
        elif word == "deref":
            yield make_token(TokenType.DEREF, None)
        elif word == "point":
            yield make_token(TokenType.POINT, None)
        elif word == "unname":
            ip += 1
            ei(
                f"No name for undefining supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            yield make_token(TokenType.UNNAME, word)
        else:
            error(
                f"Unexpected word {word!r} at position {_position} at file {file!r} while generating AST",
            )

        ip += 1


assert (
    len(TokenType) == 19
), "Unhandled token types in Linux x86_64 fasm assembly generation"
assert (
    len(Mutability) == 2
), "Unhandled mutability contexts in Linux x86_64 fasm assembly generation generation"
assert (
    len(Operator) == 6
), "Unhandled operators in Linux x86_64 fasm assembly generation generation"


def generate_assembly_linux_x86_64_fasm(
    ast: Tuple[Token, ...]
) -> Generator[Tuple[str, Token], None, None]:
    log("Getting Linux x86_64 fasm assembly from AST")

    ip: int = 0

    yield from make_usable_asm(
        ("format ELF64 executable 3", "segment readable executable")
    )

    start_lb: List[Tuple[str, Token]] = []
    data_lb: List[Tuple[str, Token]] = []
    rodata_lb: List[Tuple[str, Token]] = []

    tmp_stack: List[Any] = []

    while ip < len(ast):
        token: Token = ast[ip]
        old_ip: int = ip

        if token.ttype is None:
            error("Null token detected", token)
            break  # Because some linters are assholes

        tname: str = f"fa_{token.ttype.name}_{ip}_{len(start_lb) + len(data_lb) + len(rodata_lb)}"

        if token.ttype == TokenType.PUSH_INT:
            start_lb.extend(
                (
                    (f"mov rax, {token.tvalue}", token),
                    ("push rax", token),
                )
            )
            tmp_stack.append(token.tvalue)
        elif token.ttype == TokenType.PUSH_STR:
            parsed_string_codes: Tuple[int, ...] = tuple(
                parse_string(token.tvalue, token)
            ) or (0,)

            ip += 1
            token = ast[ip]

            str_data_lb: List[Tuple[str, Token]]

            if token.tvalue == Mutability.RW:
                str_data_lb = data_lb
            elif token.tvalue == Mutability.RO:
                str_data_lb = rodata_lb
            else:
                error(
                    "Unknown mutability context for string, did you forget to add `ro` or `rw` after the string?",
                    token,
                )

            str_data_lb.extend(
                (
                    (
                        f"str_{tname}: db {','.join(map(str, parsed_string_codes))}",
                        ast[old_ip],
                    ),
                    (f"strlen_{tname} = $ - str_{tname}", ast[old_ip]),
                )
            )

            start_lb.extend(
                (
                    (f"push strlen_{tname}", ast[old_ip]),
                    (f"push str_{tname}", ast[old_ip]),
                )
            )

            _tmp_str: str = "".join(chr(charcode) for charcode in parsed_string_codes)
            tmp_stack.extend((len(_tmp_str), _tmp_str))
        elif token.ttype == TokenType.MUTABILITY:
            error(
                "Mutability should not be parsed outside of word context, this is most likely a bug in the parser",
                token,
            )
        elif token.ttype == TokenType.SYS:
            start_lb.extend(
                (f"pop {ASM_SNIPPETS['fasm-x86_64-linux']['registers'][reg]}", token)
                for reg in range(token.tvalue)
            )
            start_lb.extend((("syscall", token), ("push rax", token)))

            # Syscall always returns
            tmp_stack.append(0)
        elif token.ttype == TokenType.DROP:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("imul rax, 8", token),
                    ("add rsp, rax", token),
                )
            )
            tmp_stack.pop()
        elif token.ttype == TokenType.SWAP:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("pop rdi", token),
                    ("push rax", token),
                    ("push rdi", token),
                )
            )

            a = tmp_stack.pop()
            b = tmp_stack.pop()

            tmp_stack.extend((a, b))
        elif token.ttype == TokenType.MACRO:
            error(
                "Macro should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.END:
            error(
                "End should not be parsed outside of word context, this is most likely a compiler bug",
                token,
            )
        elif token.ttype == TokenType.EXPAND_MACRO:
            error(
                "Macro espansion should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.INCLUDE:
            error(
                "Include should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.OPERATOR:
            if token.tvalue == Operator.ADD:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("add rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() + tmp_stack.pop())
            elif token.tvalue == Operator.SUB:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("sub rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() - tmp_stack.pop())
            elif token.tvalue == Operator.MUL:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("imul rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() * tmp_stack.pop())
            elif token.tvalue == Operator.BOR:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("or rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() | tmp_stack.pop())
            elif token.tvalue == Operator.BAND:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("and rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() & tmp_stack.pop())
            elif token.tvalue == Operator.XOR:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("xor rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() ^ tmp_stack.pop())
            else:
                error(f"Unknown operator {token}", token)
        elif token.ttype == TokenType.COPY:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("push rax", token),
                    ("push rax", token),
                )
            )

            item: Any = tmp_stack.pop()
            tmp_stack.extend((item, item))
        elif token.ttype == TokenType.UNDEFINE:
            error(
                "Undefine should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.NOP:
            start_lb.append(("nop", token))
        elif token.ttype == TokenType.AS:
            name_name: str = f"name_{token.tvalue}"

            if name_name not in CTX:
                CTX.add(name_name)
                data_lb.append((f"{name_name}: rb 8", token))

            start_lb.extend(
                (
                    ("pop rax", token),
                    ("mov rdi, rax", token),
                    (f"mov [{name_name}], rdi", token),
                )
            )

            tmp_stack.pop()
        elif token.ttype == TokenType.PUSH_NAME:
            start_lb.append((f"push name_{token.tvalue}", token))
            tmp_stack.append(1)
        elif token.ttype == TokenType.DEREF:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("mov rdi, [rax]", token),
                    ("push rdi", token),
                )
            )

            tmp_stack.pop()
            tmp_stack.append(1)
        elif token.ttype == TokenType.POINT:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("pop rdi", token),
                    ("mov [rdi], rax", token),
                )
            )

            tmp_stack.pop()
            tmp_stack.pop()
        elif token.tvalue == TokenType.UNNAME:
            error(
                "Unname should not be parsed outside of processing context, this is most likely a bug in the preprocessor",
                token,
            )
        else:
            error(f"Unexpected {token}", token)

        ip += 1

    if not start_lb:
        error("No executable code found, try adding `nop`")

    yield ("_start:", NULL_TOKEN)
    yield from start_lb

    if data_lb:
        yield "segment readable writeable", NULL_TOKEN
        yield from data_lb

    if rodata_lb:
        yield "segment readable", NULL_TOKEN
        yield from rodata_lb


def file_to_ast(file: str, logging: bool = False) -> Generator[Token, None, None]:
    if logging:
        log(f"Generating AST for {file!r}")

    with open(file, "r") as fa_code:
        for line_num, line in enumerate(fa_code, 1):
            if not line:
                continue

            yield from fa_to_ast(line, line_num, file)


def print_help() -> int:
    def ep(msg: str) -> None:
        sys.stderr.write(f" :: {msg}\n")

    help_page: Tuple[str, ...] = (
        "Usage:",
        f"   {sys.argv[0]} <file|-help> [flags...]",
        "Flags:",
        "   -no-anotate                       Don't anotate the generated assembly",
        "   -asm-flags <flag,flag,flag,...>   Flags to pass to the assembler",
        "   -help                             Prints help",
        "   -no-compile                       Only transpiles to assembly",
        "   -stdout                           Only print generated assembly",
        "   -no-logging                       Disable logging",
        "   -dbg <file>                       Where to dump debugging symbols",
        "   -run                              Run binary after compiling",
        "",
    )

    sys.stderr.write("\n".join(help_page))

    return EXIT_ER


def parse_flags(flags: List[str]) -> Dict[str, Any]:
    ip: int = 0

    flagd: Dict[str, Any] = {
        "anotate": True,
        "asm-flags": [],
        "output": os.path.basename(os.path.splitext(sys.argv[1])[0]),
        "compile": True,
        "stdout": False,
        "logging": True,
        "run": False,
    }

    if not flags:
        return flagd

    while ip < len(flags):
        flag: str = flags[ip]

        if flag == "-no-anotate":
            flagd["anotate"] = False
        elif flag == "-asm-flags":
            ip += 1
            ei("Missing comma-seperated flags to -asm-flags", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["asm-flags"].extend(flag.split(","))
        elif flag == "-help":
            sys.exit(print_help())
        elif flag == "-o":
            ip += 1
            ei("Missing output file for -o", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["output"] = flag
        elif flag == "-no-compile":
            flagd["compile"] = False
        elif flag == "-stdout":
            flagd["stdout"] = True
            flagd["compile"] = False
        elif flag == "-no-logging":
            flagd["logging"] = False
        elif flag == "-dbg":
            ip += 1
            ei("Missing output file for -dbg", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["asm-flags"].extend(("-s", repr(flag)))
        elif flag == "-run":
            flagd["run"] = True
        else:
            error(f"Unknown flag {flag!r}")

        ip += 1

    return flagd


def log(msg: str) -> None:
    if "flags.nolog" in CTX:
        return

    print(f" :: {msg}", file=sys.stderr)


def run_command_log(cmd: str, fail: bool = True) -> None:
    log(f"Running: {cmd}")

    if os.system(cmd) and fail:
        error("Command failed")


assert len(TokenType) == 19, "Unhandled token types in processing"
assert len(Mutability) == 2, "Unhandled mutability contexts in processing"
assert len(Operator) == 6, "Unhandled operators in processing"


def preprocess_ast(
    ast: Tuple[Token, ...], assembly_flavour: str, logging: bool = False
) -> Generator[Token, None, None]:
    if logging:
        log("Preprocessing AST")

    ip: int = 0

    while ip < len(ast):
        token: Token = ast[ip]

        if token.ttype == TokenType.MACRO:
            macro_body = []
            macro_name = token.tvalue

            if macro_name in MACROS:
                error(
                    f"Macro {macro_name!r} is already defined, did you try to `undefine {macro_name}`",
                    token,
                )

            while True:
                if (
                    token.ttype == TokenType.END
                    and token.tvalue == f"macro.{macro_name}"
                ):
                    break

                ip += 1
                ei(
                    "Macro definition overflow, did you forget to `end` it/all nested scopes?",
                    token,
                    ast,
                    ip,
                )

                token = ast[ip]
                macro_body.append(token)

            macro_body.pop()  # Removes 'end'
            MACROS[macro_name] = list(
                preprocess_ast(tuple(macro_body), assembly_flavour)
            )
        elif token.ttype == TokenType.EXPAND_MACRO:
            if token.tvalue not in MACROS:
                error(
                    f"Cannot expand undefined macro {token.tvalue!r}, did you define it using `macro {token.tvalue} <...> end`",
                    token,
                )

            yield from preprocess_ast(tuple(MACROS[token.tvalue]), assembly_flavour)
        elif token.ttype == TokenType.INCLUDE:
            file: str = token.tvalue

            if not os.path.isabs(file):
                for include in INCLUDE_RESOLVE_ORDER:
                    include_file: str = os.path.join(include, file)
                    if os.path.isfile(include_file) or os.path.islink(include_file):
                        file = include_file
                        break

            yield from type_check_ast(
                tuple(preprocess_ast(tuple(file_to_ast(file)), assembly_flavour)),
                assembly_flavour,
            )
        elif token.ttype == TokenType.UNDEFINE:
            if token.tvalue not in MACROS:
                error(
                    f"Macro {token.tvalue!r} is not defined, did you define it using `macro {token.tvalue} <...> end`",
                    token,
                )

            del MACROS[token.tvalue]
        elif token.ttype == TokenType.AS:
            NAMES.add(token.tvalue)
            yield token
        elif token.ttype == TokenType.PUSH_NAME:
            if token.tvalue not in NAMES:
                error(
                    f"{token.tvalue!r} is not a defined name, try something like `0 as {token.tvalue}`",
                    token,
                )

            yield token
        elif token.ttype == TokenType.UNNAME:
            if token.tvalue not in NAMES:
                error(
                    f"{token.tvalue!r} is not a defined name, try something like `0 as {token.tvalue}`",
                    token,
                )

            NAMES.remove(token.tvalue)
        else:
            yield token

        ip += 1


assert len(TokenType) == 19, "Unhandled token types in type checking"
assert len(Mutability) == 2, "Unhandled mutability contexts in type checking"
assert len(Operator) == 6, "Unhandled operators in type checking"


def type_check_ast(
    ast: Tuple[Token, ...], assembly_flavour: str, logging: bool = False
) -> Generator[Token, None, None]:
    if logging:
        log("Type checking AST")

    ip: int = 0
    type_stack: List[Tuple[TokenType, Any, Token]] = []
    ast_len: int = len(ast)

    while ip < ast_len:
        token: Token = ast[ip]

        if token.ttype == TokenType.PUSH_INT:
            type_stack.append((token.ttype, token.tvalue, token))
        elif token.ttype == TokenType.PUSH_STR:
            ip += 1
            ei(
                "Missing mutability while trying to push string, did you forget to add `ro`/`rw` after the string?",
                token,
                ast,
                ip,
            )

            old_token = token
            token = ast[ip]

            if token.ttype == TokenType.MUTABILITY:
                if token.tvalue not in (Mutability.RO, Mutability.RW):
                    error(
                        f"Unknown mutability context: {token}, valid ones are: ro, rw",
                        old_token,
                    )
            else:
                error(
                    "Unspecified mutability while tring to push a string, did you put anything than `ro` or `rw` after the string?",
                    old_token,
                )

            tmp_str: str = "".join(
                chr(charcode) for charcode in parse_string(old_token.tvalue, old_token)
            )
            type_stack.extend(
                (
                    (TokenType.PUSH_INT, len(tmp_str), token),
                    (TokenType.PUSH_STR, tmp_str, token),
                )
            )
        elif token.ttype == TokenType.SYS:
            asm_registers_len: int = len(ASM_SNIPPETS[assembly_flavour]["registers"])

            if token.tvalue > asm_registers_len:
                error(
                    f"Cannot use {token.tvalue} registers when there's only {asm_registers_len} available, \
you can at maximum only use `sys {asm_registers_len}`",
                    token,
                )

            if token.tvalue < 1:
                error("Cannot use less than 1 register", token)

            sys_type_stack_len: int = len(type_stack)

            if token.tvalue > sys_type_stack_len:
                error(
                    f"Cannot use more than {sys_type_stack_len} register values, you're trying to use {token.tvalue!r} registers, \
did you try to push more items onto the stack?",
                    token,
                )

            for _ in range(token.tvalue):
                type_stack.pop()

            type_stack.append((TokenType.PUSH_INT, 0, token))
        elif token.ttype == TokenType.DROP:
            drop_ammount: Tuple[TokenType, Any, Token] = type_stack.pop()
            drop_type_stack_len: int = len(type_stack)

            if not drop_type_stack_len:
                error("No items to drop off the stack, did you try pushing any?", token)

            if drop_ammount[0] != TokenType.PUSH_INT:
                error(
                    f"Drop expected integer as the drop ammount, did you try for e.g. `{drop_type_stack_len} drop`",
                    token,
                )

            if drop_ammount[1] > drop_type_stack_len:
                error(
                    f"Cannot drop more than {drop_type_stack_len} (I.e. the whole stack) items from the stack",
                    token,
                )

            for _ in range(drop_ammount[1]):
                type_stack.pop()
        elif token.ttype == TokenType.SWAP:
            if len(type_stack) < 2:
                error("No items to swap", token)

            a = type_stack.pop()
            b = type_stack.pop()

            type_stack.extend((a, b))
        elif token.ttype == TokenType.MACRO:
            error(
                "Macro should not be parsed outside processing, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.END:
            error(
                "End should not be parsed outside processing, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.EXPAND_MACRO:
            error(
                "Macro expansion should not be parsed outside processing, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.INCLUDE:
            error(
                "Include should not be parsed outside processing, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.OPERATOR:
            if token.tvalue == Operator.ADD:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack, try pushing more \
items onto the stack",
                        token,
                    )

                add_a: Tuple[TokenType, Any, Token] = type_stack.pop()
                add_b: Tuple[TokenType, Any, Token] = type_stack.pop()

                if add_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                if add_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                type_stack.append((TokenType.PUSH_INT, add_a[1] + add_b[1], token))
            elif token.tvalue == Operator.SUB:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack, try pushing more \
items onto the stack",
                        token,
                    )

                sub_a: Tuple[TokenType, Any, Token] = type_stack.pop()
                sub_b: Tuple[TokenType, Any, Token] = type_stack.pop()

                if sub_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                if sub_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                type_stack.append((TokenType.PUSH_INT, sub_a[1] - sub_b[1], token))
            elif token.tvalue == Operator.MUL:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack, try pushing more \
items onto the stack",
                        token,
                    )

                mul_a: Tuple[TokenType, Any, Token] = type_stack.pop()
                mul_b: Tuple[TokenType, Any, Token] = type_stack.pop()

                if mul_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                if mul_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                type_stack.append((TokenType.PUSH_INT, mul_a[1] * mul_b[1], token))
            elif token.tvalue == Operator.BOR:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack, try pushing more \
items onto the stack",
                        token,
                    )

                bor_a: Tuple[TokenType, Any, Token] = type_stack.pop()
                bor_b: Tuple[TokenType, Any, Token] = type_stack.pop()

                if bor_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer, try pushing more \
items onto the stack",
                        token,
                    )

                if bor_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                type_stack.append((TokenType.PUSH_INT, bor_a[1] | bor_b[1], token))
            elif token.tvalue == Operator.BAND:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack, try pushing more \
items onto the stack",
                        token,
                    )

                band_a: Tuple[TokenType, Any, Token] = type_stack.pop()
                band_b: Tuple[TokenType, Any, Token] = type_stack.pop()

                if band_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                if band_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                type_stack.append((TokenType.PUSH_INT, band_a[1] & band_b[1], token))
            elif token.tvalue == Operator.XOR:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack, try pushing more \
items onto the stack",
                        token,
                    )

                xor_a: Tuple[TokenType, Any, Token] = type_stack.pop()
                xor_b: Tuple[TokenType, Any, Token] = type_stack.pop()

                if xor_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                if xor_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer",
                        token,
                    )

                type_stack.append((TokenType.PUSH_INT, xor_a[1] ^ xor_b[1], token))
            else:
                error(
                    f"Unknown operator while type checking: {token.tvalue.name!r}, this is probably a bug in the AST generator",
                    token,
                )
        elif token.ttype == TokenType.COPY:
            if not len(type_stack):
                error(
                    "Cannot copy any elements from an empty stack, try pushing something on it",
                    token,
                )

            copy_element: Tuple[TokenType, Any, Token] = type_stack.pop()
            type_stack.extend((copy_element, copy_element))
        elif token.ttype == TokenType.UNDEFINE:
            error(
                "Undefine should not be parsed outside processing, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.NOP:
            pass
        elif token.ttype == TokenType.AS:
            if not len(type_stack):
                error(
                    "Please initialise the name before declaring it, did you try using `0` as the value?",
                    token,
                )

            type_stack.pop()
        elif token.ttype == TokenType.PUSH_NAME:
            type_stack.append((TokenType.PUSH_INT, 1, token))
        elif token.ttype == TokenType.DEREF:
            if not len(type_stack):
                error(
                    "No pointers to dereference on the stack, try pushing some", token
                )

            ptr_t, ptr_value, ptr_token = type_stack.pop()

            if ptr_t != TokenType.PUSH_INT:
                error(
                    "No valid pointer type found, are you sure it's an integer?",
                    ptr_token,
                )

            if ptr_value < 1:
                error("Pointer underflow detected", ptr_value)

            type_stack.append((TokenType.PUSH_INT, 1, token))
        elif token.ttype == TokenType.POINT:
            if len(type_stack) < 2:
                error(
                    "Point requires at least you arguments on the stack: a pointer and a value",
                    token,
                )

            type_stack.pop()

            new_ptr_value_t, _, new_ptr_token = type_stack.pop()

            if new_ptr_value_t != TokenType.PUSH_INT:
                error(
                    "No valid pointer type found, are you sure it's an integer?", token
                )
        elif token.ttype == TokenType.UNNAME:
            error(
                "Unname should not be parsed outside of processing context, this is most likely a bug in the preprocessor",
                token,
            )
        else:
            error(
                f"Unknown token while type checking: {token}, this is probably a bug in the AST generator",
                token,
            )

        ip += 1

    if type_stack:
        err_type_stack_len: int = len(type_stack)
        log(f"{err_type_stack_len} orphaned items on the stack")

        for item in type_stack:
            sys.stderr.write("    ")
            log(f"{item[0].name:30s}{item[1]!r} @ {item[2]}")

        error(f"Try using: {err_type_stack_len} drop")

    yield from ast


def main() -> int:
    """Entry/main function"""

    if len(sys.argv) < 2:
        return print_help()
    elif sys.argv[1] == "-help":
        print_help()
        return 0
    elif not os.path.isfile(sys.argv[1]):
        error(f"{sys.argv[1]!r}: No such file")

    flags: Dict[str, Any] = parse_flags(sys.argv[2:])

    if not flags["anotate"]:
        ANOTATION_CONF["enabled"] = False

    if not flags["logging"]:
        CTX.add("flags.nolog")

    bin_name: str = flags["output"]
    asm_name: str = f"{bin_name}.asm"

    log("Generating assembly")
    asm_content: str = "\n".join(
        anotate_assembly(
            generate_assembly_linux_x86_64_fasm(
                tuple(
                    type_check_ast(
                        tuple(
                            preprocess_ast(
                                tuple(file_to_ast(sys.argv[1], True)),
                                "fasm-x86_64-linux",
                                True,
                            )
                        ),
                        "fasm-x86_64-linux",
                        True,
                    )
                )
            ),
            ";",
        )
    )

    if flags["stdout"]:
        log("Printing generated assembly to STDOUT")
        print(asm_content)
    else:
        with open(asm_name, "w") as asm_file:
            log(f"Writting assembly to {asm_file.name!r}")
            asm_file.write(asm_content)

    if flags["compile"]:
        run_command_log(
            f"fasm {asm_name!r} {bin_name!r} {' '.join(flags['asm-flags'])} >/dev/null"
        )

        log(f"Making {bin_name!r} R(W)E")
        os.chmod(bin_name, 0o0755)

    if flags["run"]:
        run_command_log(f"./{bin_name!r}", False)

    return EXIT_OK


if __name__ == "__main__":
    assert main.__annotations__.get("return") is int, "main() should return an integer"

    filter_warnings("error", category=Warning)
    sys.exit(main())
