#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Fa programming language"""

import os
import sys
from dataclasses import dataclass
from enum import Enum, auto
from typing import (Any, Dict, Generator, Iterable, List, Optional, Set, Sized,
                    Tuple)
from warnings import filterwarnings as filter_warnings


class TokenType(Enum):
    PUSH_INT = auto()
    PUSH_STR = auto()
    MUTABILITY = auto()
    SYS = auto()
    DROP = auto()
    SWAP = auto()
    MACRO = auto()
    END = auto()
    EXPAND_MACRO = auto()
    INCLUDE = auto()
    OPERATOR = auto()
    COPY = auto()
    BUFFER = auto()
    PUSH_BUFFER = auto()
    UNDEFINE = auto()
    NOP = auto()


class Mutability(Enum):
    RW = auto()
    RO = auto()


class Operator(Enum):
    ADD = auto()
    SUB = auto()
    MUL = auto()
    BOR = auto()
    BAND = auto()
    XOR = auto()


@dataclass
class Token:
    ttype: Optional[TokenType]
    tvalue: Any
    tpos: Tuple[str, int, int]  # file, line, ip

    def __str__(self) -> str:
        if self.ttype is None:
            return self.__repr__()

        return f"Token at position {':'.join(map(str, self.tpos))!r} \
{self.ttype.name!r} with value {self.tvalue!r}"


EXIT_OK: int = 0
EXIT_ER: int = 1
NULL_TOKEN: Token = Token(None, None, ("", 0, 0))
ANOTATION_CONF: Dict[str, bool] = {"enabled": True}

ASM_SNIPPETS: Dict[str, Dict[str, Tuple[str, ...]]] = {
    "fasm-x86_64-linux": {
        "registers": (
            "rax",
            "rdi",
            "rsi",
            "rdx",
            "r8",
            "r9",
            "r10",
            "r11",
            "r12",
            "r13",
            "r14",
            "r15",
        )
    }
}

MACROS: Dict[str, List[Token]] = {}
BUFFERS: Dict[str, int] = {}
CTX: Set[str] = set()
INCLUDE_RESOLVE_ORDER: Tuple[str, ...] = (
    ".",
    "/usr/include/fa",
    "/usr/lib/fa",
    "/usr/local/include/fa",
    "/usr/local/lib/fa",
    os.path.expanduser("~/.local/include/fa"),
    os.path.expanduser("~/.local/lib/fa"),
)


def error(msg: str, token: Token = NULL_TOKEN) -> None:
    print(f"ERROR: {':'.join(map(str, token.tpos))}: {msg}", file=sys.stderr)
    sys.exit(EXIT_ER)


def ei(msg: str, token: Token, ast: Sized, ip: int) -> None:
    if ip >= len(ast):
        error(f"IP overflow: {msg}", token)


def make_usable_asm(asm: Iterable[str]) -> Generator[Tuple[str, Token], None, None]:
    yield from ((line, NULL_TOKEN) for line in asm)


def anotate_assembly(
    assembly: Generator[Tuple[str, Token], None, None], comment_char: str
) -> Generator[str, None, None]:
    log("Anotating assembly")

    for asm in assembly:
        if asm[1] == NULL_TOKEN or not ANOTATION_CONF["enabled"]:
            yield asm[0]
            continue

        yield f"    {asm[0]}  {comment_char} {asm[1]}"


def parse_string(string: str, token: Token) -> Generator[int, None, None]:
    ip: int = 0

    while ip < len(string):
        char: str = string[ip]

        if char == "\\":
            ip += 1
            ei("Invalid escape sequence", token, string, ip)

            char = string[ip]

            if char == "n":
                yield 10
            elif char == "t":
                yield 9
            elif char == "b":
                yield 8
            elif char == "f":
                yield 12
            elif char == "r":
                yield 13
            elif char == "{":
                _code: str = ""

                ip += 1
                ei("No closing } in \\{ escape", token, string, ip)

                char = string[ip]

                while char != "}":
                    _code += char
                    ip += 1
                    char = string[ip]

                if not _code:
                    error("Empty escape code", token)

                yield int(_code)
            else:
                yield ord(char)
        else:
            yield ord(char)

        ip += 1


assert len(TokenType) == 16, "Unhandled token types in AST generation"
assert len(Mutability) == 2, "Unhandled mutability contexts in AST generation"
assert len(Operator) == 6, "Unhandled operators in AST generation"


def fa_to_ast(fa: str, line: int, file: str = "") -> Generator[Token, None, None]:
    words: List[str] = fa.split(" ")
    ip: int = 0

    def make_token(ttype: TokenType, tvalue: Any) -> Token:
        return Token(ttype, tvalue, (file, line, ip))

    while ip < len(words):
        word: str = words[ip].strip()
        old_ip: int = ip
        _position: str = ":".join(map(str, (file, line, ip)))

        if not word:
            ip += 1
            continue

        if word.startswith("--"):
            break

        if word.startswith('"'):
            if word.startswith('"') and word.endswith('"') and word != '"':
                yield make_token(
                    TokenType.PUSH_STR,
                    word[1:-1],
                )
                ip += 1
                continue

            old_word = word

            ip += 1
            word = words[ip]

            _tmp_str: List[str] = [old_word, word]

            while not word.endswith('"'):
                ip += 1

                try:
                    word = words[ip].strip()
                except IndexError:
                    if len(_tmp_str) >= 2 and word != '"':
                        error(
                            f"Unclosed string at position {_position} at file {file!r} \
(starting at IP {old_ip})",
                        )

                _tmp_str.append(word)

            yield make_token(
                TokenType.PUSH_STR,
                " ".join(_tmp_str)[1:-1],
            )
        elif word.isnumeric():
            if word.startswith("0") and len(word) != 1:
                error(f"Word at position {_position}: numbers cannot start with 0")

            yield make_token(TokenType.PUSH_INT, int(word))
        elif word.startswith("%"):
            yield make_token(TokenType.EXPAND_MACRO, word[1:])
        elif word.startswith("#"):
            yield make_token(TokenType.PUSH_BUFFER, word[1:])
        elif word == "rw":
            yield make_token(TokenType.MUTABILITY, Mutability.RW)
        elif word == "ro":
            yield make_token(TokenType.MUTABILITY, Mutability.RO)
        elif word == "sys":
            ip += 1
            ei(
                f"No register ammount supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip]

            yield make_token(TokenType.SYS, int(word))
        elif word == "drop":
            yield make_token(TokenType.DROP, None)
        elif word == "swap":
            yield make_token(TokenType.SWAP, None)
        elif word == "macro":
            ip += 1
            ei(f"No macro name supplied at position {_position}", NULL_TOKEN, words, ip)

            word = words[ip].strip()

            yield make_token(TokenType.MACRO, word)
        elif word == "end":
            yield make_token(TokenType.END, None)
        elif word == "include":
            _include_file: List[str] = []

            ip += 1
            ei(
                f"No include filename supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            if not word.startswith("'"):
                error("Include must start with '")

            _include_file.append(word[1:])

            while not word.endswith("'"):
                ip += 1
                ei(
                    f"Unclosed include filename supplied at position {_position}",
                    NULL_TOKEN,
                    words,
                    ip,
                )

                word = words[ip].strip()
                _include_file.append(word)

            yield make_token(TokenType.INCLUDE, " ".join(_include_file)[:-1])
        elif word == "add":
            yield make_token(TokenType.OPERATOR, Operator.ADD)
        elif word == "sub":
            yield make_token(TokenType.OPERATOR, Operator.SUB)
        elif word == "mul":
            yield make_token(TokenType.OPERATOR, Operator.MUL)
        elif word == "copy":
            yield make_token(TokenType.COPY, None)
        elif word == "buffer":
            ip += 1
            ei(
                f"No buffer name supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip]

            yield make_token(TokenType.BUFFER, word)
        elif word == "bor":
            yield make_token(TokenType.OPERATOR, Operator.BOR)
        elif word == "band":
            yield make_token(TokenType.OPERATOR, Operator.BAND)
        elif word == "xor":
            yield make_token(TokenType.OPERATOR, Operator.XOR)
        elif word == "undefine":
            ip += 1
            ei(
                f"No macro undefinition name supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            yield make_token(TokenType.UNDEFINE, word)
        elif word == "nop":
            yield make_token(TokenType.NOP, None)
        else:
            error(
                f"Unexpected word {word!r} at position {_position} at file {file!r} while generating AST",
            )

        ip += 1


assert (
    len(TokenType) == 16
), "Unhandled token types in Linux x86_64 fasm assembly generation"
assert (
    len(Mutability) == 2
), "Unhandled mutability contexts in Linux x86_64 fasm assembly generation generation"
assert (
    len(Operator) == 6
), "Unhandled operators in Linux x86_64 fasm assembly generation generation"


def generate_assembly_linux_x86_64_fasm(
    ast: Tuple[Token, ...]
) -> Generator[Tuple[str, Token], None, None]:
    log("Getting Linux x86_64 fasm assembly from AST")

    ip: int = 0

    yield from make_usable_asm(
        ("format ELF64 executable 3", "segment readable executable")
    )

    start_lb: List[Tuple[str, Token]] = []
    data_lb: List[Tuple[str, Token]] = []
    rodata_lb: List[Tuple[str, Token]] = []
    bss_lb: List[Tuple[str, Token]] = []

    tmp_stack: List[Any] = []

    while ip < len(ast):
        token: Token = ast[ip]
        old_ip: int = ip

        if token.ttype is None:
            error("Null token detected", token)
            break  # Because some linters are assholes

        tname: str = f"fa_{token.ttype.name}_{ip}_{len(start_lb) + len(data_lb) + len(rodata_lb)}"

        if token.ttype == TokenType.PUSH_INT:
            start_lb.extend(
                (
                    (f"mov rax, {token.tvalue}", token),
                    ("push rax", token),
                )
            )
            tmp_stack.append(token.tvalue)
        elif token.ttype == TokenType.PUSH_STR:
            parsed_string_codes: Tuple[int, ...] = tuple(
                parse_string(token.tvalue, token)
            ) or (0,)

            ip += 1
            token = ast[ip]

            str_data_lb: List[Tuple[str, Token]]

            if token.tvalue == Mutability.RW:
                str_data_lb = data_lb
            elif token.tvalue == Mutability.RO:
                str_data_lb = rodata_lb
            else:
                error("Unknown mutability context for string", token)

            str_data_lb.extend(
                (
                    (
                        f"str_{tname}: db {','.join(map(str, parsed_string_codes))}",
                        ast[old_ip],
                    ),
                    (f"strlen_{tname} = $ - str_{tname}", ast[old_ip]),
                )
            )

            start_lb.extend(
                (
                    (f"push strlen_{tname}", ast[old_ip]),
                    (f"push str_{tname}", ast[old_ip]),
                )
            )

            _tmp_str: str = "".join(chr(charcode) for charcode in parsed_string_codes)
            tmp_stack.extend((len(_tmp_str), _tmp_str))
        elif token.ttype == TokenType.MUTABILITY:
            error(
                "Mutability should not be parsed outside of word context",
                token,
            )
        elif token.ttype == TokenType.SYS:
            start_lb.extend(
                (f"pop {ASM_SNIPPETS['fasm-x86_64-linux']['registers'][reg]}", token)
                for reg in range(token.tvalue)
            )
            start_lb.extend((("syscall", token), ("push rax", token)))

            # Syscall always returns
            tmp_stack.append(0)
        elif token.ttype == TokenType.DROP:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("imul rax, 8", token),
                    ("add rsp, rax", token),
                )
            )
            tmp_stack.pop()
        elif token.ttype == TokenType.SWAP:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("pop rdi", token),
                    ("push rax", token),
                    ("push rdi", token),
                )
            )

            a = tmp_stack.pop()
            b = tmp_stack.pop()

            tmp_stack.extend((a, b))
        elif token.ttype == TokenType.MACRO:
            error(
                "Macro should not be parsed outside of processing context",
                token,
            )
        elif token.ttype == TokenType.END:
            error(
                "End should not be parsed outside of word context",
                token,
            )
        elif token.ttype == TokenType.EXPAND_MACRO:
            error(
                "Macro espansion should not be parsed outside of processing context",
                token,
            )
        elif token.ttype == TokenType.INCLUDE:
            error(
                "Include should not be parsed outside of processing context",
                token,
            )
        elif token.ttype == TokenType.OPERATOR:
            if token.tvalue == Operator.ADD:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("add rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() + tmp_stack.pop())
            elif token.tvalue == Operator.SUB:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("sub rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() - tmp_stack.pop())
            elif token.tvalue == Operator.MUL:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("imul rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() * tmp_stack.pop())
            elif token.tvalue == Operator.BOR:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("or rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() | tmp_stack.pop())
            elif token.tvalue == Operator.BAND:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("and rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() & tmp_stack.pop())
            elif token.tvalue == Operator.XOR:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("xor rax, rdi", token),
                        ("push rax", token),
                    )
                )

                tmp_stack.append(tmp_stack.pop() ^ tmp_stack.pop())
            else:
                error(f"Unknown operator {token}", token)
        elif token.ttype == TokenType.COPY:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("push rax", token),
                    ("push rax", token),
                )
            )

            item: Any = tmp_stack.pop()
            tmp_stack.extend((item, item))
        elif token.ttype == TokenType.PUSH_BUFFER:
            buffer_name: str = f"buffer_{token.tvalue}"

            if buffer_name not in CTX:
                bss_lb.append((f"{buffer_name}: rb {BUFFERS[token.tvalue]}", token))
                CTX.add(buffer_name)

            start_lb.extend(
                (
                    (f"mov rax, {buffer_name}", token),
                    ("push rax", token),
                )
            )

            tmp_stack.append(0)
        elif token.ttype == TokenType.UNDEFINE:
            error(
                "Undefine should not be parsed outside of processing context",
                token,
            )
        elif token.ttype == TokenType.NOP:
            start_lb.append(("nop", token))
        else:
            error(f"Unexpected {token}", token)

        ip += 1

    if not start_lb:
        error("No code found")

    yield ("_start:", NULL_TOKEN)
    yield from start_lb

    if data_lb:
        yield "segment readable writeable", NULL_TOKEN
        yield from data_lb

    if rodata_lb:
        yield "segment readable", NULL_TOKEN
        yield from rodata_lb

    if bss_lb:
        yield "segment readable writable", NULL_TOKEN
        yield from bss_lb


def file_to_ast(file: str, logging: bool = False) -> Generator[Token, None, None]:
    if logging:
        log(f"Generating AST for {file!r}")

    with open(file, "r") as fa_code:
        for line_num, line in enumerate(fa_code, 1):
            if not line:
                continue

            yield from fa_to_ast(line, line_num, file)


def print_help() -> int:
    def ep(msg: str) -> None:
        sys.stderr.write(f" :: {msg}\n")

    help_page: Tuple[str, ...] = (
        "Usage:",
        f"   {sys.argv[0]} <file|-help> [flags...]",
        "Flags:",
        "   -no-anotate                                 Doesn't anotate assembly",
        "   -asm-flags <flag,flag,flag,...>             Sets assembly flags",
        "   -help                                       Prints help",
        "   -no-compile                                 Only transpiles to assembly",
        "   -stdout                                     Print generated assembly",
        "   -no-logging                                 Disable logging",
        "   -dbg <file>                                 Where to dump debugging symbols",
        "",
    )

    sys.stderr.write("\n".join(help_page))

    return EXIT_ER


def parse_flags(flags: List[str]) -> Dict[str, Any]:
    ip: int = 0

    flagd: Dict[str, Any] = {
        "anotate": True,
        "asm-flags": [],
        "output": os.path.basename(os.path.splitext(sys.argv[1])[0]),
        "compile": True,
        "stdout": False,
        "logging": True,
    }

    if not flags:
        return flagd

    while ip < len(flags):
        flag: str = flags[ip]

        if flag == "-no-anotate":
            flagd["anotate"] = False
        elif flag == "-asm-flags":
            ip += 1
            ei("Missing comma-seperated flags to -asm-flags", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["asm-flags"].extend(flag.split(","))
        elif flag == "-help":
            sys.exit(print_help())
        elif flag == "-o":
            ip += 1
            ei("Missing output file to -o", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["output"] = flag
        elif flag == "-no-compile":
            flagd["compile"] = False
        elif flag == "-stdout":
            flagd["stdout"] = True
            flagd["compile"] = False
        elif flag == "-no-logging":
            flagd["logging"] = False
        elif flag == "-dbg":
            ip += 1
            ei("Missing output file for -dbg", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["asm-flags"].extend(("-s", repr(flag)))
        else:
            error(f"Unknown flag {flag!r}")

        ip += 1

    return flagd


def log(msg: str) -> None:
    if "flags.nolog" in CTX:
        return

    print(f" :: {msg}", file=sys.stderr)


def run_command_log(cmd: str) -> None:
    log(f"Running: {cmd}")

    if os.system(cmd):
        error("Command failed")


assert len(TokenType) == 16, "Unhandled token types in processing"
assert len(Mutability) == 2, "Unhandled mutability contexts in processing"
assert len(Operator) == 6, "Unhandled operators in processing"


def preprocess_ast(
    ast: Tuple[Token, ...], assembly_flavour: str, logging: bool = False
) -> Generator[Token, None, None]:
    if logging:
        log("Preprocessing AST")

    ip: int = 0

    while ip < len(ast):
        token: Token = ast[ip]

        if token.ttype == TokenType.MACRO:
            macro_body = []
            macro_name = token.tvalue

            if macro_name in MACROS:
                error(f"Macro {macro_name!r} is already defined", token)

            while token.ttype != TokenType.END:
                ip += 1
                ei("Macro definition overflow", token, ast, ip)

                token = ast[ip]
                macro_body.append(token)

            macro_body.pop()  # Removes 'end'
            MACROS[macro_name] = list(
                preprocess_ast(tuple(macro_body), assembly_flavour)
            )
        elif token.ttype == TokenType.EXPAND_MACRO:
            if token.tvalue not in MACROS:
                error(f"Cannot expand undefined macro {token.tvalue!r}", token)

            yield from preprocess_ast(tuple(MACROS[token.tvalue]), assembly_flavour)
        elif token.ttype == TokenType.INCLUDE:
            file: str = token.tvalue

            if not os.path.isabs(file):
                for include in INCLUDE_RESOLVE_ORDER:
                    include_file: str = os.path.join(include, file)
                    if os.path.isfile(include_file) or os.path.islink(include_file):
                        file = include_file
                        break

            yield from type_check_ast(
                tuple(preprocess_ast(tuple(file_to_ast(file)), assembly_flavour)),
                assembly_flavour,
            )
        elif token.ttype == TokenType.BUFFER:
            buffer_name: str = token.tvalue

            ip += 1
            ei("No buffer size specified", token, ast, ip)

            token = ast[ip]
            buffer_size: int

            if token.ttype == TokenType.PUSH_INT:
                buffer_size = token.tvalue
            elif token.ttype == TokenType.EXPAND_MACRO:
                buffer_size = tuple(
                    preprocess_ast(tuple(MACROS[token.tvalue]), assembly_flavour)
                )[0].tvalue
            else:
                error(f"Unsuported buffer size: {token}", token)

            BUFFERS[buffer_name] = buffer_size
        elif token.ttype == TokenType.UNDEFINE:
            if token.tvalue not in MACROS:
                error(f"Macro {token.tvalue!r} is not defined")

            del MACROS[token.tvalue]
        else:
            yield token

        ip += 1


assert len(TokenType) == 16, "Unhandled token types in type checking"
assert len(Mutability) == 2, "Unhandled mutability contexts in type checking"
assert len(Operator) == 6, "Unhandled operators in type checking"


def type_check_ast(
    ast: Tuple[Token, ...], assembly_flavour: str, logging: bool = False
) -> Generator[Token, None, None]:
    if logging:
        log("Type checking AST")

    ip: int = 0
    type_stack: List[Tuple[TokenType, Any]] = []
    ast_len: int = len(ast)

    while ip < ast_len:
        token: Token = ast[ip]

        if token.ttype == TokenType.PUSH_INT:
            type_stack.append((token.ttype, token.tvalue))
        elif token.ttype == TokenType.PUSH_STR:
            ip += 1
            ei("Missing mutability while trying to push string", token, ast, ip)

            old_token = token
            token = ast[ip]

            if token.ttype == TokenType.MUTABILITY:
                if token.tvalue not in (Mutability.RO, Mutability.RW):
                    error(f"Unknown mutability context: {token}", old_token)
            else:
                error("Unspecified mutability while tring to push a string", old_token)

            tmp_str: str = "".join(
                chr(charcode) for charcode in parse_string(old_token.tvalue, old_token)
            )
            type_stack.extend(
                ((TokenType.PUSH_INT, len(tmp_str)), (TokenType.PUSH_STR, tmp_str))
            )
        elif token.ttype == TokenType.SYS:
            asm_registers_len: int = len(ASM_SNIPPETS[assembly_flavour]["registers"])

            if token.tvalue > asm_registers_len:
                error(
                    f"Cannot use {token.tvalue} registers when there's only {asm_registers_len} available"
                )

            if token.tvalue < 1:
                error("Cannot use less than 1 register", token)

            sys_type_stack_len: int = len(type_stack)

            if token.tvalue > sys_type_stack_len:
                error(f"Cannot use more than {sys_type_stack_len} registers", token)

            for _ in range(token.tvalue):
                type_stack.pop()

            type_stack.append((TokenType.PUSH_INT, 0))
        elif token.ttype == TokenType.DROP:
            if not len(type_stack):
                error("No items to drop off the stack", token)

            drop_ammount: Tuple[TokenType, Any] = type_stack.pop()
            drop_type_stack_len: int = len(type_stack)

            if drop_ammount[0] != TokenType.PUSH_INT:
                error("Drop expected integer as the drop ammount", token)

            if drop_ammount[1] > drop_type_stack_len:
                error(
                    f"Cannot drop more than {drop_type_stack_len} items from the stack"
                )

            for _ in range(drop_ammount[1]):
                type_stack.pop()
        elif token.ttype == TokenType.SWAP:
            if len(type_stack) < 2:
                error("No items to swap", token)

            a = type_stack.pop()
            b = type_stack.pop()

            type_stack.extend((a, b))
        elif token.ttype == TokenType.MACRO:
            error("Macro should not be parsed outside processing", token)
        elif token.ttype == TokenType.END:
            error("End should not be parsed outside processing", token)
        elif token.ttype == TokenType.EXPAND_MACRO:
            error("Macro expansion should not be parsed outside processing", token)
        elif token.ttype == TokenType.INCLUDE:
            error("Include should not be parsed outside processing", token)
        elif token.ttype == TokenType.OPERATOR:
            if token.tvalue == Operator.ADD:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack",
                        token,
                    )

                add_a: Tuple[TokenType, Any] = type_stack.pop()
                add_b: Tuple[TokenType, Any] = type_stack.pop()

                if add_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer"
                    )

                if add_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer"
                    )

                type_stack.append((TokenType.PUSH_INT, add_a[1] + add_b[1]))
            elif token.tvalue == Operator.SUB:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack",
                        token,
                    )

                sub_a: Tuple[TokenType, Any] = type_stack.pop()
                sub_b: Tuple[TokenType, Any] = type_stack.pop()

                if sub_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer"
                    )

                if sub_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer"
                    )

                type_stack.append((TokenType.PUSH_INT, sub_a[1] - sub_b[1]))
            elif token.tvalue == Operator.MUL:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack",
                        token,
                    )

                mul_a: Tuple[TokenType, Any] = type_stack.pop()
                mul_b: Tuple[TokenType, Any] = type_stack.pop()

                if mul_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer"
                    )

                if mul_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer"
                    )

                type_stack.append((TokenType.PUSH_INT, mul_a[1] * mul_b[1]))
            elif token.tvalue == Operator.BOR:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack",
                        token,
                    )

                bor_a: Tuple[TokenType, Any] = type_stack.pop()
                bor_b: Tuple[TokenType, Any] = type_stack.pop()

                if bor_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer"
                    )

                if bor_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer"
                    )

                type_stack.append((TokenType.PUSH_INT, bor_a[1] | bor_b[1]))
            elif token.tvalue == Operator.BAND:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack",
                        token,
                    )

                band_a: Tuple[TokenType, Any] = type_stack.pop()
                band_b: Tuple[TokenType, Any] = type_stack.pop()

                if band_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer"
                    )

                if band_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer"
                    )

                type_stack.append((TokenType.PUSH_INT, band_a[1] & band_b[1]))
            elif token.tvalue == Operator.XOR:
                if len(type_stack) < 2:
                    error(
                        f"Operator {token.tvalue.name!r} requires at least two arguments on the stack",
                        token,
                    )

                xor_a: Tuple[TokenType, Any] = type_stack.pop()
                xor_b: Tuple[TokenType, Any] = type_stack.pop()

                if xor_a[0] != TokenType.PUSH_INT:
                    error(
                        f"First argument of operator {token.tvalue.name!r} should be an integer"
                    )

                if xor_b[0] != TokenType.PUSH_INT:
                    error(
                        f"Second argument of operator {token.tvalue.name!r} should be an integer"
                    )

                type_stack.append((TokenType.PUSH_INT, xor_a[1] ^ xor_b[1]))
            else:
                error(
                    f"Unknown operator while type checking: {token.tvalue.name!r}",
                    token,
                )
        elif token.ttype == TokenType.COPY:
            if not len(type_stack):
                error(
                    "Cannot copy any elements from an empty stack",
                    token,
                )

            copy_element: Tuple[TokenType, Any] = type_stack.pop()
            type_stack.extend((copy_element, copy_element))
        elif token.ttype == TokenType.BUFFER:
            error("Buffer should not be parsed outside processing", token)
        elif token.ttype == TokenType.PUSH_BUFFER:
            if token.tvalue not in BUFFERS:
                error(f"Buffer {token.tvalue!r} does not exist")

            type_stack.append((TokenType.PUSH_INT, 1))
        elif token.ttype == TokenType.UNDEFINE:
            error("Undefine should not be parsed outside processing", token)
        elif token.ttype == TokenType.NOP:
            pass
        else:
            error(f"Unknown token while type checking: {token}", token)

        ip += 1

    if type_stack:
        log("Unhandled items on the stack")

        for item in type_stack:
            sys.stderr.write("    ")
            log(str(item))

        sys.exit(EXIT_ER)

    yield from ast


def main() -> int:
    """Entry/main function"""

    if len(sys.argv) < 2:
        return print_help()
    elif sys.argv[1] == "-help":
        print_help()
        return 0
    elif not os.path.isfile(sys.argv[1]):
        error(f"{sys.argv[1]!r}: No such file")

    flags: Dict[str, Any] = parse_flags(sys.argv[2:])

    if not flags["anotate"]:
        ANOTATION_CONF["enabled"] = False

    if not flags["logging"]:
        CTX.add("flags.nolog")

    bin_name: str = flags["output"]
    asm_name: str = f"{bin_name}.asm"

    log("Generating assembly")
    asm_content: str = "\n".join(
        anotate_assembly(
            generate_assembly_linux_x86_64_fasm(
                tuple(
                    type_check_ast(
                        tuple(
                            preprocess_ast(
                                tuple(file_to_ast(sys.argv[1], True)),
                                "fasm-x86_64-linux",
                                True,
                            )
                        ),
                        "fasm-x86_64-linux",
                        True,
                    )
                )
            ),
            ";",
        )
    )

    if flags["stdout"]:
        log("Printing generated assembly to STDOUT")
        print(asm_content)
    else:
        with open(asm_name, "w") as asm_file:
            log(f"Writting assembly to {asm_file.name!r}")
            asm_file.write(asm_content)

    if flags["compile"]:
        run_command_log(
            f"fasm {asm_name!r} {bin_name!r} {' '.join(flags['asm-flags'])}"
        )

        run_command_log(f"chmod a+rx -- {bin_name!r}")

    return EXIT_OK


if __name__ == "__main__":
    assert main.__annotations__.get("return") is int, "main() should return an integer"

    filter_warnings("error", category=Warning)
    sys.exit(main())
