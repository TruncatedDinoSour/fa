#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Fa programming language"""

import os
import sys
from dataclasses import dataclass
from enum import Enum, auto
from typing import (Any, Callable, Dict, Generator, Iterable, List, Optional,
                    Set, Sized, Tuple)
from warnings import filterwarnings as filter_warnings


class TokenType(Enum):
    PUSH_INT = auto()
    PUSH_STR = auto()
    MUTABILITY = auto()
    SYS = auto()
    DROP = auto()
    SWAP = auto()
    MACRO = auto()
    END = auto()
    EXPAND_MACRO = auto()
    INCLUDE = auto()
    OPERATOR = auto()
    COPY = auto()
    UNDEFINE = auto()
    NOP = auto()
    AS = auto()
    PUSH_NAME = auto()
    DEREF = auto()
    POINT = auto()
    UNNAME = auto()
    SHIFT = auto()
    IF = auto()
    END_IF = auto()
    ELSE = auto()
    LABEL = auto()
    GOTO = auto()
    SET = auto()
    RESET = auto()
    FUN = auto()
    FUN_END = auto()
    FUN_CALL = auto()
    RET = auto()
    ELIPSIS = auto()
    TYPE = auto()
    BOOL = auto()
    NULL = auto()
    SIG_START = auto()
    SIG_END = auto()
    EO = auto()
    ARGC = auto()
    ARGV = auto()


class Mutability(Enum):
    RW = auto()
    RO = auto()


class Operator(Enum):
    ADD = auto()
    SUB = auto()
    MUL = auto()
    BOR = auto()
    BAN = auto()
    XOR = auto()
    SHR = auto()
    SHL = auto()
    GRT = auto()
    LST = auto()
    BOT = auto()
    EQL = auto()


class Type(Enum):
    INT = auto()
    STR = auto()
    PTR = auto()
    BUL = auto()
    NUL = auto()


class Bool(Enum):
    TRUE = auto()
    FALSE = auto()


@dataclass
class TypeSignature:
    input_types: list[Type]
    output_types: list[Type]


@dataclass
class Token:
    ttype: Optional[TokenType]
    tvalue: Any
    tpos: Tuple[str, int, int]  # file, line, ip

    def __str__(self) -> str:
        if self.ttype is None:
            return self.__repr__()

        return f"Token {self.ttype.name!r} at position {':'.join(map(str, self.tpos))!r} \
with value {self.tvalue!r}"


EXIT_OK: int = 0
EXIT_ER: int = 1
NULL_TOKEN: Token = Token(None, None, ("", 0, 0))
ANOTATION_CONF: Dict[str, bool] = {"enabled": True}

ASM_SNIPPETS: Dict[str, Dict[str, Tuple[str, ...]]] = {
    "fasm-x86_64-linux": {
        "registers": (
            "rax",
            "rdi",
            "rsi",
            "rdx",
            "r8",
            "r9",
            "r10",
            "r11",
            "r12",
            "r13",
            "r14",
            "r15",
        )
    }
}

MACROS: Dict[str, List[Token]] = {}
FUNCTIONS: List[str] = []
UNSAFE_CHARS: str = "`~!@#$^&*()+{}[];:|\\<>?/'\""
CTX: Set[str] = set()
INCLUDE_RESOLVE_ORDER: Tuple[str, ...] = (
    ".",
    "/usr/include/fa",
    "/usr/lib/fa",
    "/usr/local/include/fa",
    "/usr/local/lib/fa",
    os.path.expanduser("~/.local/include/fa"),
    os.path.expanduser("~/.local/lib/fa"),
)
SCOPES: List[str] = []
NAMES: Set[str] = set()
GLOBAL_STATES: Dict[str, Any] = {
    "shift": 0,
    "anot_passes": 0,
    "asm_passes": 0,
    "tc_passes": 0,
    "pp_passes": 0,
    "fin_passes": 0,
    "if": 0,
    "icc": 0,
    "rstack-size": 8096,
}
INCLUDED_FILES: Set[str] = set()

COLOURS: Dict[str, str] = {
    "red": "31m",
    "yellow": "33m",
    "reset": "0m",
}


def log(msg: str, header: str = "\033[1m::\033[0m", file=sys.stdout) -> None:
    if "flags.nolog" in CTX:
        return

    print(f" {header} {msg}", file=file)


def make_log(
    header: str, position: tuple[str, int, int], msg: str, colour: str, file
) -> None:
    log(
        f"{header.upper()}:\033[{COLOURS['reset']} {':'.join(map(str, position))}: {msg}",
        f"\033[1m\033[{COLOURS.get(colour)}::",
        file=file,
    )


def error(msg: str, token: Token = NULL_TOKEN) -> None:
    make_log("ERROR", token.tpos, msg, "red", sys.stderr)
    sys.exit(EXIT_ER)


def warning(msg: str, token: Token = NULL_TOKEN) -> None:
    make_log("WARNING", token.tpos, msg, "yellow", sys.stderr)


def ei(msg: str, token: Token, ast: Sized, ip: int) -> None:
    if ip >= len(ast):
        error(f"IP overflow: {msg}", token)


def make_usable_asm(asm: Iterable[str]) -> Generator[Tuple[str, Token], None, None]:
    yield from ((line, NULL_TOKEN) for line in asm)


def check_name(name: str, position: str) -> None:
    GLOBAL_STATES["icc"] += 1

    for name_chr_idx, name_chr in enumerate(name):
        if name_chr in UNSAFE_CHARS:
            error(
                f"Illegal char at index {name_chr_idx} in name {name!r} at position {position!r}: {name_chr!r}",
                NULL_TOKEN,
            )


def anotate_assembly(
    assembly: Generator[Tuple[str, Token], None, None], comment_char: str
) -> Generator[str, None, None]:
    log("Anotating assembly")
    GLOBAL_STATES["anot_passes"] += 1

    for asm in assembly:
        if asm[1] == NULL_TOKEN or not ANOTATION_CONF["enabled"]:
            yield asm[0]
            continue

        yield f"    {asm[0]}  {comment_char} {asm[1]}"


def parse_string(string: str, token: Token) -> Generator[int, None, None]:
    ip: int = 0

    while ip < len(string):
        char: str = string[ip]

        if char == "\\":
            ip += 1
            ei("Invalid escape sequence", token, string, ip)

            char = string[ip]

            if char == "0":
                yield 0
            elif char == "b":
                yield 8
            elif char == "t":
                yield 9
            elif char == "n":
                yield 10
            elif char == "f":
                yield 12
            elif char == "r":
                yield 13
            elif char == "{":
                _code: str = ""

                while char != "}":
                    ip += 1
                    ei("Unterminated \\{ escape sequence", token, string, ip)
                    char = string[ip]

                    if char == "}":
                        break

                    _code += char

                    if not char.isnumeric():
                        error(
                            f"Invalid character {char!r} in \\{{ escape sequence", token
                        )

                if not _code:
                    error("No escape code provided", token)

                yield int(_code)
            elif char == "[":
                yield from (27, 91)

                _escape: str = ""

                while char != "]":
                    ip += 1
                    ei("Unterminated ANSI escape sequence", token, string, ip)
                    char = string[ip]

                    if char == "]":
                        break

                    if char == ";" and not _escape:
                        error("; cannot be a stand-alone escape character", token)

                    _escape += char

                    if False and not char.isnumeric() and char != ";":
                        error(
                            f"Invalid character {char!r} in ANSI escape sequence", token
                        )

                if not _escape:
                    error("No escape code provided", token)

                yield from map(ord, _escape)
            else:
                yield ord(char)
        else:
            yield ord(char)

        ip += 1


assert len(TokenType) == 40, "Unhandled token types in AST generation"
assert len(Mutability) == 2, "Unhandled mutability contexts in AST generation"
assert len(Operator) == 12, "Unhandled operators in AST generation"
assert len(Type) == 5, "Unhandled types in AST generation"
assert len(Bool) == 2, "Unhandled booleans in AST generation"


def fa_to_ast(fa: str, line: int, file: str = "") -> Generator[Token, None, None]:
    words: List[str] = fa.split(" ")
    ip: int = 0

    def make_token(ttype: TokenType, tvalue: Any) -> Token:
        return Token(ttype, tvalue, (file, line, ip))

    while ip < len(words):
        word: str = words[ip].strip()
        old_ip: int = ip
        _position: str = ":".join(map(str, (file, line, ip)))

        if not word:
            ip += 1
            continue

        if word.startswith("--<") and word.endswith(">--"):
            ip += 1
            continue

        if "comment" in CTX and word.endswith(">--"):
            CTX.remove("comment")
            ip += 1
            continue

        if "comment" in CTX:
            ip += 1
            continue

        if word.startswith("--<") and "comment" not in CTX:
            CTX.add("comment")
            ip += 1
            continue

        if word.startswith("--"):
            break

        if word.startswith('"'):
            if word.startswith('"') and word.endswith('"') and word != '"':
                yield make_token(
                    TokenType.PUSH_STR,
                    word[1:-1],
                )
                ip += 1
                continue

            old_word = word

            ip += 1
            ei(
                f'Unclosed string at position: {_position}, did you forget to add a " at the end?',
                NULL_TOKEN,
                words,
                ip,
            )
            word = words[ip]

            _tmp_str: List[str] = [old_word, word]

            while not word.endswith('"'):
                ip += 1

                try:
                    word = words[ip].strip()
                except IndexError:
                    if len(_tmp_str) >= 2 and word != '"':
                        error(
                            f'Unclosed string at position {_position} at file {file!r} \
(starting at IP {old_ip}), did you forget to add a " at the end?',
                        )

                _tmp_str.append(word)

            yield make_token(
                TokenType.PUSH_STR,
                " ".join(_tmp_str)[1:-1],
            )
        elif word.isnumeric():
            if word.startswith("0") and len(word) != 1:
                error(
                    f"Word at position {_position}: numbers cannot start with 0, did you mean {word.lstrip('0')}"
                )

            yield make_token(TokenType.PUSH_INT, int(word))
        elif word.startswith("-"):
            word = word[1:]

            if not word.isnumeric():
                error(f"Invalid negative number at position {_position}: {word!r}")

            yield make_token(TokenType.PUSH_INT, -int(word))
        elif word.startswith("%"):
            check_name(word[1:], _position)
            yield make_token(TokenType.EXPAND_MACRO, word[1:])
        elif word.startswith("@"):
            check_name(word[1:], _position)
            yield make_token(TokenType.PUSH_NAME, word[1:])
        elif word.startswith("#"):
            check_name(word[1:], _position)
            yield make_token(TokenType.FUN_CALL, f"fun_{word[1:]}")
        elif word == "rw":
            yield make_token(TokenType.MUTABILITY, Mutability.RW)
        elif word == "ro":
            yield make_token(TokenType.MUTABILITY, Mutability.RO)
        elif word == "sys":
            ip += 1
            ei(
                f"No register ammount supplied at position {_position}, did you forget to supply a number that is at least a 1",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            if not word.isnumeric():
                error(
                    f"{word!r} is not a valid register ammount at position {_position}"
                )

            yield make_token(TokenType.SYS, int(word))
        elif word == "drop":
            yield make_token(TokenType.DROP, None)
        elif word == "swap":
            yield make_token(TokenType.SWAP, None)
        elif word == "macro":
            ip += 1
            ei(
                f"No macro name for definition supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()
            check_name(word, _position)
            _macro_name: str = f"macro{word}{_position}{ip}"

            SCOPES.append(_macro_name)
            yield make_token(TokenType.MACRO, (word, _macro_name))
        elif word == "end":
            if not SCOPES:
                error(
                    f"No scope(s) to end at position {_position}, did you mean for e.g. 'macro'?",
                    NULL_TOKEN,
                )

            yield make_token(TokenType.END, SCOPES.pop())
        elif word == "include":
            _include_file: List[str] = []

            ip += 1
            ei(
                f"No include filename supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()

            if not word.startswith("'"):
                error("Include must start with '")

            _include_file.append(word[1:])

            while not word.endswith("'"):
                ip += 1
                ei(
                    f"Unclosed include filename supplied at position {_position}, did you forget to add a '",
                    NULL_TOKEN,
                    words,
                    ip,
                )

                word = words[ip].strip()
                _include_file.append(word)

            yield make_token(TokenType.INCLUDE, " ".join(_include_file)[:-1])
        elif word == "add":
            yield make_token(TokenType.OPERATOR, Operator.ADD)
        elif word == "sub":
            yield make_token(TokenType.OPERATOR, Operator.SUB)
        elif word == "mul":
            yield make_token(TokenType.OPERATOR, Operator.MUL)
        elif word == "copy":
            yield make_token(TokenType.COPY, None)
        elif word == "bor":
            yield make_token(TokenType.OPERATOR, Operator.BOR)
        elif word == "ban":
            yield make_token(TokenType.OPERATOR, Operator.BAN)
        elif word == "xor":
            yield make_token(TokenType.OPERATOR, Operator.XOR)
        elif word == "shr":
            yield make_token(TokenType.OPERATOR, Operator.SHR)
        elif word == "shl":
            yield make_token(TokenType.OPERATOR, Operator.SHL)
        elif word == "grt":
            yield make_token(TokenType.OPERATOR, Operator.GRT)
        elif word == "lst":
            yield make_token(TokenType.OPERATOR, Operator.LST)
        elif word == "bot":
            yield make_token(TokenType.OPERATOR, Operator.BOT)
        elif word == "eql":
            yield make_token(TokenType.OPERATOR, Operator.EQL)
        elif word == "undefine":
            ip += 1
            msg = f"No macro name for undefinition supplied at position {_position}"

            if len(MACROS):
                msg += f", did you mean `undefine {tuple(MACROS.keys())[-1]}"

            ei(
                msg,
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()
            check_name(word, _position)

            yield make_token(TokenType.UNDEFINE, word)
        elif word == "nop":
            yield make_token(TokenType.NOP, None)
        elif word == "as":
            ip += 1
            ei(
                f"No name for defining supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()
            check_name(word, _position)

            yield make_token(TokenType.AS, word)
        elif word == "deref":
            yield make_token(TokenType.DEREF, None)
        elif word == "point":
            yield make_token(TokenType.POINT, None)
        elif word == "unname":
            ip += 1
            ei(
                f"No name for undefining supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()
            check_name(word, _position)

            yield make_token(TokenType.UNNAME, word)
        elif word == "shift":
            yield make_token(TokenType.SHIFT, None)
        elif word == "if":
            if_name: str = f"if_{GLOBAL_STATES['if']}"

            SCOPES.append(if_name)
            yield make_token(TokenType.IF, if_name)
            GLOBAL_STATES["if"] += 1
        elif word == "else":
            yield make_token(TokenType.ELSE, SCOPES[-1])
        elif word == "label":
            ip += 1
            ei(
                f"No label name supplied for definition at position {_position}, try `label name`",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()
            label_name: str = word
            check_name(label_name, _position)

            yield make_token(TokenType.LABEL, f"label_{label_name}")
        elif word == "goto":
            ip += 1
            ei(
                f"No label name supplied to goto at position {_position}, add one after",
                NULL_TOKEN,
                words,
                ip,
            )
            word = words[ip].strip()

            goto_label_name: str = word
            check_name(goto_label_name, _position)

            yield make_token(TokenType.GOTO, f"label_{goto_label_name}")
        elif word == "set":
            yield make_token(TokenType.SET, None)
        elif word == "reset":
            yield make_token(TokenType.RESET, None)
        elif word == "fun":
            ip += 1
            ei(
                f"No function name for definition supplied at position {_position}",
                NULL_TOKEN,
                words,
                ip,
            )

            word = words[ip].strip()
            check_name(word, _position)
            _fun_name: str = f"fun{word}{_position}{ip}"

            SCOPES.append(_fun_name)
            yield make_token(TokenType.FUN, (word, _fun_name))
        elif word == "ret":
            yield make_token(TokenType.RET, None)
        elif word == "...":
            yield make_token(TokenType.ELIPSIS, None)
        elif word == "int":
            yield make_token(TokenType.TYPE, Type.INT)
        elif word == "str":
            yield make_token(TokenType.TYPE, Type.STR)
        elif word == "ptr":
            yield make_token(TokenType.TYPE, Type.PTR)
        elif word == "bul":
            yield make_token(TokenType.TYPE, Type.BUL)
        elif word == "nul":
            yield make_token(TokenType.TYPE, Type.NUL)
        elif word == "true":
            yield make_token(TokenType.BOOL, Bool.TRUE)
        elif word == "false":
            yield make_token(TokenType.BOOL, Bool.FALSE)
        elif word == "null":
            yield make_token(TokenType.NULL, None)
        elif word == "[":
            yield make_token(TokenType.SIG_START, None)
        elif word == "]":
            yield make_token(TokenType.SIG_END, None)
        elif word == "eo":
            yield make_token(TokenType.EO, None)
        elif word == "argc":
            yield make_token(TokenType.ARGC, None)
        elif word == "argv":
            yield make_token(TokenType.ARGV, None)
        else:
            error(
                f"Unexpected word {word!r} at position {_position} at file {file!r} while generating AST",
            )

        ip += 1


assert len(TokenType) == 40, "Unhandled token types in x86_64 fasm assembly generation"
assert (
    len(Mutability) == 2
), "Unhandled mutability contexts in x86_64 fasm assembly generation generation"
assert (
    len(Operator) == 12
), "Unhandled operators in x86_64 fasm assembly generation generation"
assert len(Type) == 5, "Unhandled types in x86_64 fasm assembly generation"
assert len(Bool) == 2, "Unhandled booleans in x86_64 fasm assembly generation"


def generate_assembly_x86_64_fasm(
    ast: Tuple[Token, ...]
) -> Generator[Tuple[str, Token], None, None]:
    log("Getting Linux x86_64 fasm assembly from AST")
    GLOBAL_STATES["asm_passes"] += 1

    ip: int = 0

    yield ("segment readable executable", NULL_TOKEN)

    start_lb: List[Tuple[str, Token]] = []
    data_lb: List[Tuple[str, Token]] = []
    rodata_lb: List[Tuple[str, Token]] = []

    while ip < len(ast):
        token: Token = ast[ip]
        old_ip: int = ip

        if token.ttype is None:
            error("Null token detected", token)
            break  # Because some linters are assholes

        tname: str = f"fa_{token.ttype.name}_{ip}_{len(start_lb) + len(data_lb) + len(rodata_lb)}"

        if token.ttype == TokenType.PUSH_INT:
            start_lb.extend(
                (
                    (f"mov rax, {token.tvalue}", token),
                    ("push rax", token),
                )
            )
        elif token.ttype == TokenType.PUSH_STR:
            parsed_string_codes: Tuple[int, ...] = tuple(
                parse_string(token.tvalue, token)
            ) or (0,)

            ip += 1
            token = ast[ip]

            str_data_lb: List[Tuple[str, Token]]

            if token.tvalue == Mutability.RW:
                str_data_lb = data_lb
            elif token.tvalue == Mutability.RO:
                str_data_lb = rodata_lb
            else:
                error(
                    "Unknown mutability context for string, did you forget to add `ro` or `rw` after the string?",
                    token,
                )

            str_data_lb.extend(
                (
                    (
                        f"str_{tname}: db {','.join(map(str, parsed_string_codes))}",
                        ast[old_ip],
                    ),
                    (f"strlen_{tname} = $ - str_{tname}", ast[old_ip]),
                )
            )

            start_lb.extend(
                (
                    (f"push strlen_{tname}", ast[old_ip]),
                    (f"push str_{tname}", ast[old_ip]),
                )
            )
        elif token.ttype == TokenType.MUTABILITY:
            error(
                "Mutability should not be parsed outside of word context, this is most likely a bug in the parser",
                token,
            )
        elif token.ttype == TokenType.SYS:
            start_lb.extend(
                (f"pop {ASM_SNIPPETS['fasm-x86_64-linux']['registers'][reg]}", token)
                for reg in range(token.tvalue)
            )
            start_lb.extend((("syscall", token), ("push rax", token)))
        elif token.ttype == TokenType.DROP:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("imul rax, 8", token),
                    ("add rsp, rax", token),
                )
            )
        elif token.ttype == TokenType.SWAP:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("pop rdi", token),
                    ("push rax", token),
                    ("push rdi", token),
                )
            )
        elif token.ttype == TokenType.MACRO:
            error(
                "Macro should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.END:
            error(
                "End should not be parsed outside of word context, this is most likely a compiler bug",
                token,
            )
        elif token.ttype == TokenType.EXPAND_MACRO:
            error(
                "Macro espansion should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.INCLUDE:
            error(
                "Include should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.OPERATOR:
            if token.tvalue == Operator.ADD:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("add rax, rdi", token),
                        ("push rax", token),
                    )
                )
            elif token.tvalue == Operator.SUB:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("sub rax, rdi", token),
                        ("push rax", token),
                    )
                )
            elif token.tvalue == Operator.MUL:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("imul rax, rdi", token),
                        ("push rax", token),
                    )
                )
            elif token.tvalue == Operator.BOR:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("or rax, rdi", token),
                        ("push rax", token),
                    )
                )
            elif token.tvalue == Operator.BAN:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("and rax, rdi", token),
                        ("push rax", token),
                    )
                )
            elif token.tvalue == Operator.XOR:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("pop rdi", token),
                        ("xor rax, rdi", token),
                        ("push rax", token),
                    )
                )
            elif token.tvalue == Operator.SHR:
                start_lb.extend(
                    (
                        ("pop rbx", token),
                        ("pop rcx", token),
                        ("shr rbx, cl", token),
                        ("push rbx", token),
                    )
                )
            elif token.tvalue == Operator.SHL:
                start_lb.extend(
                    (
                        ("pop rbx", token),
                        ("pop rcx", token),
                        ("shl rbx, cl", token),
                        ("push rbx", token),
                    )
                )
            elif token.tvalue == Operator.GRT:
                start_lb.extend(
                    (
                        ("mov rcx, 0", token),
                        ("mov rdx, 1", token),
                        ("pop rax", token),
                        ("pop rbx", token),
                        ("cmp rax, rbx", token),
                        ("cmovg rcx, rdx", token),
                        ("push rcx", token),
                    )
                )
            elif token.tvalue == Operator.LST:
                start_lb.extend(
                    (
                        ("mov rcx, 0", token),
                        ("mov rdx, 1", token),
                        ("pop rax", token),
                        ("pop rbx", token),
                        ("cmp rax, rbx", token),
                        ("cmovl rcx, rdx", token),
                        ("push rcx", token),
                    )
                )
            elif token.tvalue == Operator.EQL:
                start_lb.extend(
                    (
                        ("mov rcx, 0", token),
                        ("mov rdx, 1", token),
                        ("pop rax", token),
                        ("pop rbx", token),
                        ("cmp rax, rbx", token),
                        ("cmove rcx, rdx", token),
                        ("push rcx", token),
                    )
                )
            elif token.tvalue == Operator.BOT:
                start_lb.extend(
                    (
                        ("pop rax", token),
                        ("not rax", token),
                        ("push rax", token),
                    )
                )
            else:
                error(f"Unknown operator {token}", token)
        elif token.ttype == TokenType.COPY:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("push rax", token),
                    ("push rax", token),
                )
            )
        elif token.ttype == TokenType.UNDEFINE:
            error(
                "Undefine should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.NOP:
            start_lb.append(("nop", token))
        elif token.ttype == TokenType.AS:
            name_name: str = f"name_{token.tvalue}"

            if name_name not in CTX:
                CTX.add(name_name)
                data_lb.append((f"{name_name}: rq 1", token))

            start_lb.extend(
                (
                    ("pop rax", token),
                    ("mov rdi, rax", token),
                    (f"mov [{name_name}], rdi", token),
                )
            )
        elif token.ttype == TokenType.PUSH_NAME:
            start_lb.append((f"push name_{token.tvalue}", token))
        elif token.ttype == TokenType.DEREF:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("mov rdi, [rax]", token),
                    ("push rdi", token),
                )
            )
        elif token.ttype == TokenType.POINT:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("pop rdi", token),
                    ("mov [rdi], rax", token),
                )
            )
        elif token.tvalue == TokenType.UNNAME:
            error(
                "Unname should not be parsed outside of processing context, this is most likely a bug in the preprocessor",
                token,
            )
        elif token.ttype == TokenType.SHIFT:
            error(
                "Shift should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.IF:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("test rax, rax", token),
                    (f"jz {token.tvalue}", token),
                )
            )
        elif token.ttype == TokenType.END_IF:
            start_lb.append((f"{token.tvalue}:", NULL_TOKEN))
        elif token.ttype == TokenType.ELSE:
            start_lb.extend(
                (
                    (f"jmp {token.tvalue}", token),
                    (f"else_{token.tvalue}:", NULL_TOKEN),
                )
            )
        elif token.ttype == TokenType.LABEL:
            start_lb.append((f"{token.tvalue}:", NULL_TOKEN))
        elif token.ttype == TokenType.GOTO:
            start_lb.append((f"jmp {token.tvalue}", token))
        elif token.ttype == TokenType.SET:
            start_lb.extend(
                (
                    ("pop rax", token),
                    ("pop rdi", token),
                    ("mov qword [rdi], rax", token),
                )
            )
        elif token.ttype == TokenType.RESET:
            error(
                "Reset should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.FUN:
            if "fun_data" not in CTX:
                start_lb.insert(0, ("mov [rstack_rsp], rax", token))
                start_lb.insert(0, ("mov rax, rstack_end", token))

                data_lb.extend(
                    (
                        ("rstack_rsp: rq 1", token),
                        (f"rstack: rb {GLOBAL_STATES['rstack-size']}", token),
                        ("rstack_end:", token),
                    )
                )
                CTX.add("fun_data")

            start_lb.extend(
                (
                    (f"jmp {token.tvalue[2]}", token),
                    (f"{token.tvalue[1]}:", NULL_TOKEN),
                    ("mov [rstack_rsp], rsp", token),
                    ("mov rsp, rax", token),
                )
            )
        elif token.ttype == TokenType.FUN_END:
            start_lb.append(
                (f"{token.tvalue}:", NULL_TOKEN),
            )
        elif token.ttype == TokenType.RET:
            if "fun_data" not in CTX:
                start_lb.insert(0, ("mov [rstack_rsp], rax", token))
                start_lb.insert(0, ("mov rax, rstack_end", token))

                data_lb.extend(
                    (
                        ("rstack_rsp: rq 1", token),
                        (f"rstack: rb {GLOBAL_STATES['rstack-size']}", token),
                        ("rstack_end:", token),
                    )
                )
                CTX.add("fun_data")

            start_lb.extend(
                (
                    ("mov rax, rsp", token),
                    ("mov rsp, [rstack_rsp]", token),
                    ("ret", token),
                )
            )
        elif token.ttype == TokenType.FUN_CALL:
            if "fun_data" not in CTX:
                start_lb.insert(0, ("mov [rstack_rsp], rax", token))
                start_lb.insert(0, ("mov rax, rstack_end", token))

                data_lb.extend(
                    (
                        ("rstack_rsp: rq 1", token),
                        (f"rstack: rb {GLOBAL_STATES['rstack-size']}", token),
                        ("rstack_end:", token),
                    )
                )
                CTX.add("fun_data")

            start_lb.extend(
                (
                    ("mov rax, rsp", token),
                    ("mov rsp, [rstack_rsp]", token),
                    (f"call {token.tvalue}", token),
                    ("mov [rstack_rsp], rsp", token),
                    ("mov rsp, rax", token),
                )
            )
        elif token.ttype == TokenType.ELIPSIS:
            error(
                "Elipsis should not be parsed outside of processing context, this is most likely a bug in the (pre-)processor",
                token,
            )
        elif token.ttype == TokenType.TYPE:
            error(
                "Types are only supposed to be used for type checking, you either placed one outside a valid context \
or it's a bug in pre-processor and/or type checker",
                token,
            )
        elif token.ttype == TokenType.BOOL:
            if token.tvalue == Bool.TRUE:
                start_lb.append(("push 1", token))
            elif token.tvalue == Bool.FALSE:
                start_lb.append(("push 0", token))
            else:
                error(f"Unknown boolean: {token}", token)
        elif token.ttype == TokenType.NULL:
            start_lb.append(("push 0", token))
        elif token.ttype == TokenType.SIG_START:
            error(
                "Type signatures are not supposed to be parsed outside type checking and pre-processing, might be a bug in the type checker",
                token,
            )
        elif token.ttype == TokenType.SIG_END:
            error(
                "Stray type signature end found",
                token,
            )
        elif token.ttype == TokenType.EO:
            error(
                "EO (End Output) is only supposed to be used as a delimeter, might be a bug in the pre-processor",
                token,
            )
        elif token.ttype == TokenType.ARGV:
            if "cli_data" not in CTX:
                start_lb.insert(0, ("mov [cli_ptr], rsp", token))
                data_lb.append(("cli_ptr: rq 1", token))

                CTX.add("cli_data")

            start_lb.extend(
                (
                    ("mov rax, [cli_ptr]", token),
                    ("add rax, 8", token),
                    ("push rax", token),
                )
            )
        elif token.ttype == TokenType.ARGC:
            if "cli_data" not in CTX:
                start_lb.insert(0, ("mov [cli_ptr], rsp", token))
                data_lb.append(("cli_ptr: rq 1", token))

                CTX.add("cli_data")

            start_lb.extend(
                (
                    ("mov rax, [cli_ptr]", token),
                    ("mov rax, [rax]", token),
                    ("push rax", token),
                )
            )
        else:
            error(f"Unexpected {token}", token)

        ip += 1

    if not start_lb:
        error("No executable code found, try adding `nop`")

    yield ("_start:", NULL_TOKEN)
    yield from start_lb

    if data_lb:
        yield "segment readable writeable", NULL_TOKEN
        yield from data_lb

    if rodata_lb:
        yield "segment readable", NULL_TOKEN
        yield from rodata_lb


def generate_assembly_gnulinux_x86_64_fasm(
    ast: Tuple[Token, ...]
) -> Generator[Tuple[str, Token], None, None]:
    yield ("format ELF64 executable 3", NULL_TOKEN)
    yield from generate_assembly_x86_64_fasm(ast)


def generate_assembly_freebsd_x86_64_fasm(
    ast: Tuple[Token, ...]
) -> Generator[Tuple[str, Token], None, None]:
    yield ("format ELF64 executable 9", NULL_TOKEN)
    yield from generate_assembly_x86_64_fasm(ast)


FORMATS: Dict[str, Callable] = {
    "x86-64-gnulinux": generate_assembly_gnulinux_x86_64_fasm,
    "x86-64-freebsd": generate_assembly_freebsd_x86_64_fasm,
}


def file_to_ast(file: str, logging: bool = False) -> Generator[Token, None, None]:
    if logging:
        log(f"Generating AST for {file!r}")

    with open(file, "r") as fa_code:
        for line_num, line in enumerate(fa_code, 1):
            if not line:
                continue

            yield from fa_to_ast(line, line_num, file)


def print_help() -> int:
    def ep(msg: str) -> None:
        sys.stderr.write(f" :: {msg}\n")

    help_page: Tuple[str, ...] = (
        "Usage:",
        f"   {sys.argv[0]} <file> [flags...]",
        "Flags:",
        "   -no-anotate                       Don't anotate the generated assembly",
        "   -asm-flags <flag,flag,flag,...>   Flags to pass to the assembler",
        "   -help                             Prints help",
        "   -no-compile                       Only transpiles to assembly",
        "   -stdout                           Only print generated assembly",
        "   -no-logging                       Disable logging",
        "   -dbg <file>                       Where to dump debugging symbols",
        "   -run                              Run binary after compiling",
        "   -list-formats                     List all formats",
        "   -format <format>                  What format of assembly to generate",
        "   -input <file>                     Set input file",
        "   -output <file>                    Set output file",
        "   -rstack-size <size>               Set return stack size in bytes",
        "   -flags <flag,flag,flag>           The arguments to pass to the binary when -run is specified",
        "",
    )

    sys.stderr.write("\n".join(help_page))

    return EXIT_ER


def parse_flags(flags: List[str]) -> Dict[str, Any]:
    ip: int = 0

    flagd: Dict[str, Any] = {
        "anotate": True,
        "asm-flags": [],
        "output": os.path.basename(os.path.splitext(sys.argv[1])[0]),
        "compile": True,
        "stdout": False,
        "logging": True,
        "run": False,
        "format": "x86-64-gnulinux",
        "input": None,
        "flags": [],
    }

    if not flags:
        sys.exit(print_help())

    while ip < len(flags):
        flag: str = flags[ip]

        if not flag.startswith("-"):
            flagd["input"] = flag
            ip += 1
            continue

        if flag == "-no-anotate":
            flagd["anotate"] = False
        elif flag == "-asm-flags":
            ip += 1
            ei("Missing comma-seperated flags to -asm-flags", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["asm-flags"].extend(flag.split(","))
        elif flag == "-help":
            print_help()
            sys.exit(EXIT_OK)
        elif flag == "-output":
            ip += 1
            ei("Missing output file for -output", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["output"] = flag
        elif flag == "-no-compile":
            flagd["compile"] = False
        elif flag == "-stdout":
            flagd["stdout"] = True
            flagd["compile"] = False
        elif flag == "-no-logging":
            flagd["logging"] = False
        elif flag == "-dbg":
            ip += 1
            ei("Missing output file for -dbg", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["asm-flags"].extend(("-s", flag))
        elif flag == "-run":
            flagd["run"] = True
        elif flag == "-list-formats":
            log("All available formats:")

            for fmt in FORMATS.keys():
                sys.stdout.write("    ")
                sys.stdout.flush()

                if fmt == flagd["format"]:
                    fmt = f"{fmt} *"

                log(fmt)

            sys.exit(EXIT_OK)
        elif flag == "-format":
            ip += 1
            ei("Missing format specified for -format", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            if flag not in FORMATS:
                error(f"Unknown format: {flag!r}")

            flagd["format"] = flag
        elif flag == "-input":
            ip += 1
            ei("Missing input file for -input", NULL_TOKEN, flags, ip)

            flag = flags[ip]
            flagd["input"] = flag
        elif flag == "-rstack-size":
            ip += 1
            ei("Missing return stack size for -rstack-size", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            if not flag.isnumeric():
                error("Invalid return stack size in bytes for -rstack-size: {flag!r}")

            GLOBAL_STATES["rstack-size"] = int(flag)
        elif flag == "-flags":
            ip += 1
            ei("Missing comma-seperated flags to -flags", NULL_TOKEN, flags, ip)

            flag = flags[ip]

            flagd["flags"].extend(flag.split(","))
        else:
            error(f"Unknown flag {flag!r}")

        ip += 1

    if not os.path.isfile(flagd["input"]):
        error("No valid input file specified")

    return flagd


def run_command_log(cmd: Tuple[str, ...]) -> None:
    pid: int = os.fork()

    if pid < 0:
        error("fork() failed")
    elif pid == 0:
        log(f"Executing in child: {' '.join(cmd)!r}")

        os.dup2(os.open(os.devnull, os.O_WRONLY | os.O_CREAT | os.O_TRUNC), 1)

        if os.execvp(cmd[0], cmd) == -1:
            error("Command failed!")
    else:
        if os.waitpid(pid, 0)[1] != EXIT_OK:
            sys.exit(EXIT_ER)


assert len(TokenType) == 40, "Unhandled token types in processing"
assert len(Mutability) == 2, "Unhandled mutability contexts in processing"
assert len(Operator) == 12, "Unhandled operators in processing"
assert len(Type) == 5, "Unhandled types in processing"
assert len(Bool) == 2, "Unhandled booleans in processing"


def preprocess_ast(
    ast: Tuple[Token, ...], assembly_flavour: str, logging: bool = False
) -> Generator[Token, None, None]:
    if logging:
        log("Preprocessing AST")

    GLOBAL_STATES["pp_passes"] += 1

    ip: int = 0

    while ip < len(ast):
        token: Token = ast[ip]

        if token.ttype == TokenType.MACRO:
            macro_body = []
            macro_name, macro_id = token.tvalue

            # if macro_name in MACROS:
            #     error(
            #         f"Macro {macro_name!r} is already defined, did you try to `undefine {macro_name}`",
            #         token,
            #     )

            while True:
                if token.ttype == TokenType.END and token.tvalue == macro_id:
                    break

                ip += 1
                ei(
                    "Macro definition overflow, did you forget to `end` it/all nested scopes?",
                    token,
                    ast,
                    ip,
                )

                token = ast[ip]
                macro_body.append(token)

            macro_body.pop()  # Removes 'end'
            MACROS[macro_name] = list(
                preprocess_ast(tuple(macro_body), assembly_flavour)
            )
        elif token.ttype == TokenType.EXPAND_MACRO:
            if token.tvalue not in MACROS:
                error(
                    f"Cannot expand undefined macro {token.tvalue!r}, did you define it using `macro {token.tvalue} <...> end`",
                    token,
                )

            yield from preprocess_ast(tuple(MACROS[token.tvalue]), assembly_flavour)
        elif token.ttype == TokenType.INCLUDE:
            file: str = token.tvalue

            if not os.path.isabs(file):
                for include in INCLUDE_RESOLVE_ORDER:
                    include_file: str = os.path.join(include, file)
                    if os.path.isfile(include_file) or os.path.islink(include_file):
                        file = include_file
                        break

            file = os.path.abspath(file)

            if not os.path.exists(file):
                error(f"Cannot include a non-existant file {file!r}", token)

            if file in INCLUDED_FILES:
                ip += 1
                continue

            INCLUDED_FILES.add(file)

            yield from type_check_ast(
                tuple(preprocess_ast(tuple(file_to_ast(file)), assembly_flavour)),
                assembly_flavour,
            )
        elif token.ttype == TokenType.UNDEFINE:
            if token.tvalue not in MACROS:
                error(
                    f"Macro {token.tvalue!r} is not defined, did you define it using `macro {token.tvalue} <...> end`",
                    token,
                )

            del MACROS[token.tvalue]
        elif token.ttype == TokenType.SHIFT:
            yield Token(TokenType.PUSH_INT, GLOBAL_STATES["shift"], token.tpos)
            GLOBAL_STATES["shift"] += 1
        elif token.ttype == TokenType.INCLUDE:
            pass
        elif token.ttype == TokenType.RESET:
            GLOBAL_STATES["shift"] = 0
        elif token.ttype == TokenType.FUN:
            fun_body = []
            signature: Dict[str, list[Type]] = {"in": [], "out": []}
            signed = False
            signature_input = False
            fun_token: Token = token
            fun_name, fun_id = fun_token.tvalue

            if fun_name in FUNCTIONS:
                error(f"Function {fun_name!r} is already defined", token)

            while True:
                if token.ttype == TokenType.END and token.tvalue == fun_id:
                    break

                ip += 1
                ei(
                    "Function definition overflow, did you forget to `end` it/all nested scopes?",
                    token,
                    ast,
                    ip,
                )
                token = ast[ip]

                if not signed:
                    if token.ttype == TokenType.EO:
                        signed = True
                        continue

                    if token.ttype == TokenType.SIG_START:
                        signature_input = True
                        continue

                    if token.ttype == TokenType.SIG_END:
                        if not signature_input:
                            error(
                                "Trying to end an unstarted input type signature", token
                            )

                        signature_input = False
                        continue

                    if token.ttype != TokenType.TYPE:
                        error(
                            "Invalid types in funtion type signature, did you maybe forgot to end the type signature ([ or eo)",
                            token,
                        )

                    if signature_input:
                        signature["in"].append(token.tvalue)
                    else:
                        signature["out"].append(token.tvalue)

                    continue
                else:
                    if signature_input:
                        error("Unclosed type signature, did you forget ]?", token)

                    fun_body.append(token)

            fun_body.pop()  # Removes 'end'

            FUNCTIONS.append(fun_name)

            yield Token(
                TokenType.FUN,
                (
                    TypeSignature(
                        input_types=signature["in"], output_types=signature["out"]
                    ),
                    f"fun_{fun_name}",
                    f"end_fun_{fun_name}",
                ),
                fun_token.tpos,
            )
            yield from preprocess_ast(tuple(fun_body), assembly_flavour, False)
            yield Token(TokenType.RET, None, fun_token.tpos)
            yield Token(TokenType.FUN_END, f"end_fun_{fun_name}", fun_token.tpos)
        elif token.ttype == TokenType.ELIPSIS:
            pass
        else:
            yield token

        ip += 1


def type_check_ast(
    ast: Tuple[Token, ...], assembly_flavour: str, logging: bool = False
) -> Generator[Token, None, None]:
    if logging:
        warning("Type checking currently has been removed")

    GLOBAL_STATES["tc_passes"] += 1
    yield from ast

    # Yes there used to be code here, but I removed it


assert len(TokenType) == 40, "Unhandled token types in AST validation"
assert len(Mutability) == 2, "Unhandled mutability contexts in AST validation"
assert len(Operator) == 12, "Unhandled operators in AST validation"
assert len(Type) == 5, "Unhandled types in AST validation"
assert len(Bool) == 2, "Unhandled booleans in AST validation"


def finalise_ast(
    ast: Tuple[Token, ...], /, in_if: bool = False, do_log: bool = True
) -> Generator[Token, None, None]:
    if do_log:
        log("Finalising the AST")

    GLOBAL_STATES["fin_passes"] += 1

    ip: int = 0
    token: Token

    while ip < len(ast):
        token = ast[ip]

        if token.ttype == TokenType.AS:
            if token.tvalue in NAMES:
                error(
                    f"Name {token.tvalue!r} is already defined, try `unname {token.tvalue}`",
                    token,
                )

            NAMES.add(token.tvalue)
            yield token
        elif token.ttype == TokenType.PUSH_NAME:
            if token.tvalue not in NAMES:
                error(
                    f"{token.tvalue!r} is not a defined name, try something like `0 as {token.tvalue}`",
                    token,
                )
            yield token
        elif token.ttype == TokenType.UNNAME:
            if token.tvalue not in NAMES:
                error(
                    f"{token.tvalue!r} is not a defined name, try something like `0 as {token.tvalue}`",
                    token,
                )

            NAMES.remove(token.tvalue)
        elif token.ttype == TokenType.IF:
            if_body = []
            if_token = token
            if_jmp = token.tvalue

            while True:
                if token.ttype == TokenType.END and token.tvalue == if_token.tvalue:
                    break

                if token.ttype == TokenType.ELSE and token.tvalue == if_token.tvalue:
                    if_jmp = f"else_{token.tvalue}"

                ip += 1
                ei(
                    "If conditional overflow, did you forget to `end` it/all nested scopes?",
                    token,
                    ast,
                    ip,
                )

                token = ast[ip]
                if_body.append(token)

            if_body.pop()  # Removes 'end'

            yield Token(TokenType.IF, if_jmp, if_token.tpos)
            yield from finalise_ast(tuple(if_body), in_if=True, do_log=False)
            yield Token(TokenType.END_IF, if_token.tvalue, if_token.tpos)
        elif token.ttype == TokenType.ELSE and not in_if:
            error(f"Token {token} should not be parsed outside an if block", token)
        else:
            yield token

        ip += 1


def main() -> int:
    """Entry/main function"""

    if len(sys.argv) < 2:
        return print_help()

    flags: Dict[str, Any] = parse_flags(sys.argv[1:])

    if not flags["anotate"]:
        ANOTATION_CONF["enabled"] = False

    if not flags["logging"]:
        CTX.add("flags.nolog")

    bin_name: str = flags["output"]
    asm_name: str = f"{bin_name}.asm"

    log("Generating assembly")
    asm_content: str = "\n".join(
        anotate_assembly(
            FORMATS[flags["format"]](
                tuple(
                    type_check_ast(
                        tuple(
                            finalise_ast(
                                tuple(
                                    preprocess_ast(
                                        tuple(file_to_ast(flags["input"], True)),
                                        "fasm-x86_64-linux",
                                        True,
                                    )
                                ),
                            )
                        ),
                        "fasm-x86_64-linux",
                        True,
                    )
                )
            ),
            ";",
        )
    )

    if flags["stdout"]:
        log("Printing generated assembly to STDOUT")
        print(asm_content)
    else:
        with open(asm_name, "w") as asm_file:
            log(f"Writting assembly to {asm_file.name!r}")
            asm_file.write(asm_content)

    if flags["compile"]:
        run_command_log(("fasm", asm_name, bin_name, *flags["asm-flags"]))

        log(f"Making {bin_name!r} R(W)E")
        os.chmod(bin_name, 0o0755)

    log("Compilation finished")

    log("", "")
    log(f"Anotation passes              {GLOBAL_STATES['anot_passes']}", " ")
    log(f"Assembly geneneration passes  {GLOBAL_STATES['asm_passes']}", " ")
    log(f"Type checking passes          {GLOBAL_STATES['tc_passes']}", " ")
    log(f"Preprocessing passes          {GLOBAL_STATES['pp_passes']}", " ")
    log(f"Finalising passes             {GLOBAL_STATES['fin_passes']}", " ")
    log(f"Illegal character checking    {GLOBAL_STATES['icc']}", " ")

    if flags["run"]:
        log("", "")
        log("Executing the binary (replacing current process)")
        log("", "")

        os.execv(bin_name, (bin_name, *flags["flags"]))

    return EXIT_OK


if __name__ == "__main__":
    assert main.__annotations__.get("return") is int, "main() should return an integer"

    filter_warnings("error", category=Warning)
    sys.exit(main())
